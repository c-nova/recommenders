{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xDeepFM : the eXtreme Deep Factorization Machine \n",
    "このノートブックでは、[xDeepFM モデル](https://arxiv.org/abs/1803.05170)をトレーニングする方法の簡単な例を示します。\n",
    "xDeepFM [1] は、低次から高次のフィーチャ インタラクションの両方をキャプチャすることを目的とした、ディープラーニングベースのモデルを使用した精密なレコメンデーション システムです。したがって、フィーチャーの相互作用をより効果的に学習でき、手動でのフィーチャー エンジニアリングの労力を大幅に削減できます。要約すると、xDeepFM には次の主要な要素があります:\n",
    "* CIN という名前のコンポーネントが含まれており、フィーチャの相互作用を明示的な方法でベクトル単位で学習します。\n",
    "* 暗黙的な方法でビット単位でフィーチャーの相互作用を学習する、従来の DNN コンポーネントが含まれています。\n",
    "* このモデルは非常に構成変更が可能な実装です。`use_Linear_part`、 `use_FM_part`、 `use_CIN_part` 及び `use_DNN_part` などのハイパーパラメーターを設定することで、コンポーネントの異なるサブセットを有効にできます。たとえば、`use_Linear_part` と `use_FM_part` のみを有効にするとで、古典的な FM モデルを取得できます。\n",
    "\n",
    "このノートブックでは、1) 小さな合成データセットと 2) [Criteo データセット](http://labs.criteo.com/category/dataset)の 2 つのデータセットで xDeepFM をテストできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. グローバル設定とインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.11 | packaged by conda-forge | (default, Aug  5 2020, 20:09:42) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from reco_utils.common.constants import SEED\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import (\n",
    "    download_deeprec_resources, prepare_hparams\n",
    ")\n",
    "from reco_utils.recommender.deeprec.models.xDeepFM import XDeepFMModel\n",
    "from reco_utils.recommender.deeprec.io.iterator import FFMTextIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "EPOCHS_FOR_SYNTHETIC_RUN = 15\n",
    "EPOCHS_FOR_CRITEO_RUN = 10\n",
    "BATCH_SIZE_SYNTHETIC = 128\n",
    "BATCH_SIZE_CRITEO = 4096\n",
    "RANDOM_SEED = SEED  # 非確定的な結果にする際には None を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xDeepFM はデータ入力として 次のようなFFM 形式を使用します: `<label> <field_id>:<feature_id>:<feature_value>`\n",
    "各行はインスタンスを表し、`<label>` では 1 は正のインスタンスを意味し、0 は負のインスタンスを意味するバイナリ値です。\n",
    "フィーチャはフィールドに分割されます。例えば、ユーザの性別はフィールドであり、男性、女性および不明の3つの選択可能な値を含みます。職業は、性別フィールドよりも多くの選択可能な値を含む別のフィールドにすることができます。フィールド インデックスとフィーチャ インデックスの両方が 1 から始まります。<br>\n",
    "\n",
    "## 1.合成データ\n",
    "それでは、小さな合成データセットから始めましょう。このデータセットには、10 のフィールド、1000 のフィーチャーがあり、一連の事前設定されたペアごとのフィーチャの相互作用の結果に従ってラベルが生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.3k/10.3k [00:01<00:00, 5.24kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "yaml_file = os.path.join(data_path, r'xDeepFM.yaml')\n",
    "train_file = os.path.join(data_path, r'synthetic_part_0')\n",
    "valid_file = os.path.join(data_path, r'synthetic_part_1')\n",
    "test_file = os.path.join(data_path, r'synthetic_part_2')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/deeprec/', data_path, 'xdeepfmresources.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 ハイパーパラメータの準備\n",
    "prepare_hparams() は、学習率、フィーチャ数、ドロップアウト率など、モデルトレーニング用のハイパーパラメータの完全なセットを作成します。これらのパラメータを yaml ファイルに入れたり、関数のパラメーターとしてパラメータを渡したりできます (yaml 設定を上書きします)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kg_file=None,user_clicks=None,FEATURE_COUNT=1000,FIELD_COUNT=10,data_format=ffm,PAIR_NUM=None,DNN_FIELD_NUM=None,n_user=None,n_item=None,n_user_attr=None,n_item_attr=None,iterator_type=None,SUMMARIES_DIR=None,MODEL_DIR=None,wordEmb_file=None,entityEmb_file=None,contextEmb_file=None,news_feature_file=None,user_history_file=None,use_entity=True,use_context=True,doc_size=None,history_size=None,word_size=None,entity_size=None,entity_dim=None,entity_embedding_method=None,transform=None,train_ratio=None,dim=10,layer_sizes=[100, 100],cross_layer_sizes=[1],cross_layers=None,activation=['relu', 'relu'],cross_activation=identity,user_dropout=False,dropout=[0.0, 0.0],attention_layer_sizes=None,attention_activation=None,attention_dropout=0.0,model_type=xDeepFM,method=classification,load_saved_model=False,load_model_name=you model path,filter_sizes=None,num_filters=None,mu=None,fast_CIN_d=0,use_Linear_part=False,use_FM_part=False,use_CIN_part=True,use_DNN_part=False,init_method=tnormal,init_value=0.3,embed_l2=0.0001,embed_l1=0.0,layer_l2=0.0001,layer_l1=0.0,cross_l2=0.0001,cross_l1=0.0,reg_kg=0.0,learning_rate=0.001,lr_rs=1,lr_kg=0.5,kg_training_interval=5,max_grad_norm=2,is_clip_norm=0,dtype=32,loss=log_loss,optimizer=adam,epochs=15,batch_size=128,enable_BN=False,show_step=200000,save_model=False,save_epoch=2,metrics=['auc', 'logloss'],write_tfevents=False,item_embedding_dim=None,cate_embedding_dim=None,user_embedding_dim=None,train_num_ngs=4,need_sample=True,embedding_dropout=0.0,user_vocab=None,item_vocab=None,cate_vocab=None,pairwise_metrics=None,EARLY_STOP=100,max_seq_length=None,hidden_size=None,L=None,T=None,n_v=None,n_h=None,min_seq_length=1,attention_size=None,att_fcn_layer_sizes=None,dilations=None,kernel_size=None,embed_size=None,n_layers=None,decay=None,eval_epoch=None,top_k=None\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          FEATURE_COUNT=1000, \n",
    "                          FIELD_COUNT=10, \n",
    "                          cross_l2=0.0001, \n",
    "                          embed_l2=0.0001, \n",
    "                          learning_rate=0.001, \n",
    "                          epochs=EPOCHS_FOR_SYNTHETIC_RUN,\n",
    "                          batch_size=BATCH_SIZE_SYNTHETIC)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 データローダーの作成\n",
    "モデルのデータ用イテレータを指定します。xDeepFM は FFMTextIterator を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = FFMTextIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 モデルの作成\n",
    "ハイパーパラメータとデータ イテレータの両方の準備ができたら、モデルを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add CIN part.\n"
     ]
    }
   ],
   "source": [
    "model = XDeepFMModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "## モデルを最初からトレーニングしたくない場合は、次のような\n",
    "## 事前トレーニング済みのモデルを読み込むことができます:\n",
    "#model.load_model(r'your_model_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、この時点でのモデルのパフォーマンスを見てみましょう (トレーニングを開始せずに):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.5043, 'logloss': 0.7515}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC=0.5 はランダム推論の状態です。トレーニングの前に、モデルがランダムな推測のように振る舞うことが確認できます。\n",
    "\n",
    "#### 1.4 モデルのトレーニング\n",
    "次に、トレーニング セットでモデルをトレーニングし、検証データセットのパフォーマンスを確認します。モデルのトレーニングは、関数呼び出しと同じくらい簡単です:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:0.7556826068773302\n",
      "eval info: auc:0.504, logloss:0.7042\n",
      "at epoch 1 , train time: 4.3 eval time: 0.6\n",
      "at epoch 2\n",
      "train info: logloss loss:0.7263523231666932\n",
      "eval info: auc:0.5066, logloss:0.6973\n",
      "at epoch 2 , train time: 4.3 eval time: 0.8\n",
      "at epoch 3\n",
      "train info: logloss loss:0.7177084291104189\n",
      "eval info: auc:0.5099, logloss:0.6953\n",
      "at epoch 3 , train time: 3.8 eval time: 0.7\n",
      "at epoch 4\n",
      "train info: logloss loss:0.7118660186983875\n",
      "eval info: auc:0.5147, logloss:0.6946\n",
      "at epoch 4 , train time: 3.7 eval time: 0.8\n",
      "at epoch 5\n",
      "train info: logloss loss:0.7055103289302682\n",
      "eval info: auc:0.523, logloss:0.6941\n",
      "at epoch 5 , train time: 3.8 eval time: 0.7\n",
      "at epoch 6\n",
      "train info: logloss loss:0.6954095556154284\n",
      "eval info: auc:0.5416, logloss:0.6929\n",
      "at epoch 6 , train time: 3.6 eval time: 0.8\n",
      "at epoch 7\n",
      "train info: logloss loss:0.6723950118133702\n",
      "eval info: auc:0.5916, logloss:0.6831\n",
      "at epoch 7 , train time: 4.0 eval time: 0.7\n",
      "at epoch 8\n",
      "train info: logloss loss:0.6119807973964927\n",
      "eval info: auc:0.7024, logloss:0.6288\n",
      "at epoch 8 , train time: 4.2 eval time: 0.8\n",
      "at epoch 9\n",
      "train info: logloss loss:0.5020270644594303\n",
      "eval info: auc:0.8154, logloss:0.5257\n",
      "at epoch 9 , train time: 4.3 eval time: 0.8\n",
      "at epoch 10\n",
      "train info: logloss loss:0.38994721433346213\n",
      "eval info: auc:0.8826, logloss:0.4315\n",
      "at epoch 10 , train time: 4.4 eval time: 0.7\n",
      "at epoch 11\n",
      "train info: logloss loss:0.30144522643785704\n",
      "eval info: auc:0.9205, logloss:0.3605\n",
      "at epoch 11 , train time: 4.4 eval time: 0.6\n",
      "at epoch 12\n",
      "train info: logloss loss:0.23429049589892023\n",
      "eval info: auc:0.9431, logloss:0.3082\n",
      "at epoch 12 , train time: 4.2 eval time: 0.6\n",
      "at epoch 13\n",
      "train info: logloss loss:0.18283964168677216\n",
      "eval info: auc:0.9577, logloss:0.2682\n",
      "at epoch 13 , train time: 3.9 eval time: 0.7\n",
      "at epoch 14\n",
      "train info: logloss loss:0.14280925843467826\n",
      "eval info: auc:0.9676, logloss:0.2369\n",
      "at epoch 14 , train time: 3.9 eval time: 0.7\n",
      "at epoch 15\n",
      "train info: logloss loss:0.11175788457655825\n",
      "eval info: auc:0.9745, logloss:0.2128\n",
      "at epoch 15 , train time: 3.7 eval time: 0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7f9d74f7ff60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 モデルの評価\n",
    "\n",
    "ここでも、モデルのパフォーマンスを見てみましょう (トレーニング後):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.9716, 'logloss': 0.2278}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file)\n",
    "print(res_syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": {
        "auc": 0.9716,
        "logloss": 0.2278
       },
       "encoder": "json",
       "name": "res_syn",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "res_syn"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.glue(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価メトリックではなく完全な予測スコアを取得する場合は、次の操作で行うことができます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7f9d74f7ff60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criteo データ \n",
    "\n",
    "これで、合成データセットの実験に成功しました。次に、実世界のデータセットとして [Criteo データセット](http://labs.criteo.com/category/dataset)の小さなサンプルで何かを試してみましょう。Criteo データセットは、CTR 予測モデルを開発するための業界のベンチマーク データセットとしてよく知られており、研究論文による評価データセットとして頻繁に採用されています。元のデータセットは軽量のデモでは大きすぎるため、デモ データセットとしてそのデータセットの一部をサンプリングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo with Criteo dataset\n"
     ]
    }
   ],
   "source": [
    "print('Criteo データセットによるデモ')\n",
    "hparams = prepare_hparams(yaml_file, \n",
    "                          FEATURE_COUNT=2300000, \n",
    "                          FIELD_COUNT=39, \n",
    "                          cross_l2=0.01, \n",
    "                          embed_l2=0.01, \n",
    "                          layer_l2=0.01,\n",
    "                          learning_rate=0.002, \n",
    "                          batch_size=BATCH_SIZE_CRITEO, \n",
    "                          epochs=EPOCHS_FOR_CRITEO_RUN, \n",
    "                          cross_layer_sizes=[20, 10], \n",
    "                          init_value=0.1, \n",
    "                          layer_sizes=[20,20],\n",
    "                          use_Linear_part=True, \n",
    "                          use_CIN_part=True, \n",
    "                          use_DNN_part=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(data_path, r'cretio_tiny_train')\n",
    "valid_file = os.path.join(data_path, r'cretio_tiny_valid')\n",
    "test_file = os.path.join(data_path, r'cretio_tiny_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add linear part.\n",
      "Add CIN part.\n",
      "Add DNN part.\n"
     ]
    }
   ],
   "source": [
    "model = XDeepFMModel(hparams, FFMTextIterator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.4728, 'logloss': 0.7113}\n"
     ]
    }
   ],
   "source": [
    "# モデルをトレーニングする前に予測パフォーマンスを確認する\n",
    "print(model.run_eval(test_file)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:744.3602027893066\n",
      "eval info: auc:0.6637, logloss:0.5342\n",
      "at epoch 1 , train time: 21.7 eval time: 4.3\n",
      "at epoch 2\n",
      "train info: logloss loss:385.66927337646484\n",
      "eval info: auc:0.7137, logloss:0.5109\n",
      "at epoch 2 , train time: 21.4 eval time: 4.3\n",
      "at epoch 3\n",
      "train info: logloss loss:191.50830841064453\n",
      "eval info: auc:0.7283, logloss:0.5037\n",
      "at epoch 3 , train time: 21.4 eval time: 4.2\n",
      "at epoch 4\n",
      "train info: logloss loss:92.20774269104004\n",
      "eval info: auc:0.7359, logloss:0.4991\n",
      "at epoch 4 , train time: 21.6 eval time: 4.4\n",
      "at epoch 5\n",
      "train info: logloss loss:43.159456968307495\n",
      "eval info: auc:0.74, logloss:0.4963\n",
      "at epoch 5 , train time: 21.6 eval time: 4.3\n",
      "at epoch 6\n",
      "train info: logloss loss:19.656921446323395\n",
      "eval info: auc:0.7426, logloss:0.4946\n",
      "at epoch 6 , train time: 21.3 eval time: 4.2\n",
      "at epoch 7\n",
      "train info: logloss loss:8.77035716176033\n",
      "eval info: auc:0.7441, logloss:0.4934\n",
      "at epoch 7 , train time: 21.5 eval time: 4.3\n",
      "at epoch 8\n",
      "train info: logloss loss:3.9227354377508163\n",
      "eval info: auc:0.7453, logloss:0.4925\n",
      "at epoch 8 , train time: 21.7 eval time: 4.3\n",
      "at epoch 9\n",
      "train info: logloss loss:1.8598770573735237\n",
      "eval info: auc:0.7462, logloss:0.4917\n",
      "at epoch 9 , train time: 21.3 eval time: 4.2\n",
      "at epoch 10\n",
      "train info: logloss loss:1.0249397158622742\n",
      "eval info: auc:0.747, logloss:0.491\n",
      "at epoch 10 , train time: 21.5 eval time: 4.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7f9d64b4a2e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7356, 'logloss': 0.5017}\n"
     ]
    }
   ],
   "source": [
    "# モデルをトレーニングした後の予測パフォーマンスを確認する\n",
    "res_real = model.run_eval(test_file)\n",
    "print(res_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": {
        "auc": 0.7356,
        "logloss": 0.5017
       },
       "encoder": "json",
       "name": "res_real",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "res_real"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.glue(\"res_real\", res_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クリーンアップ\n",
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\\[1\\] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., & Sun, G. (2018). xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, KDD 2018, London, UK, August 19-23, 2018.<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (reco_gpu)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
