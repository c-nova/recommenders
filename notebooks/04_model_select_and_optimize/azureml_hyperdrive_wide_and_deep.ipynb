{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br>\n",
    "# Wide-and-Deep Model Hyperparameter Tuning with AzureML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to auto-tune hyperparameters of a recommender model by utilizing **Azure Machine Learning service** ([AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/))<sup><a href=\"#azureml-search\">a</a>, <a href=\"#azure-subscription\">b</a></sup>.\n",
    "\n",
    "We present an overall process of utilizing AzureML, specifically [**Hyperdrive**](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py) component, for the hyperparameter tuning by demonstrating key steps:\n",
    "1. Configure AzureML Workspace\n",
    "2. Create Remote Compute Target (GPU cluster)\n",
    "3. Prepare Data\n",
    "4. Prepare Training Scripts\n",
    "5. Setup and Run Hyperdrive Experiment\n",
    "6. Model Import, Re-train and Test\n",
    "\n",
    "In this notebook, we use [**Wide-and-Deep model**](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) from **TensorFlow high-level Estimator API (v1.12 or higher)** on the movie recommendation scenario. Wide-and-Deep learning jointly trains wide linear model and deep neural networks (DNN) to combine the benefits of memorization and generalization for recommender systems.\n",
    "\n",
    "For more details about the **Wide-and-Deep** model:\n",
    "* [Wide-and-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb)\n",
    "* [Original paper](https://arxiv.org/abs/1606.07792)\n",
    "* [TensorFlow API doc](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor)\n",
    "  \n",
    "Regarding **AuzreML**, please refer:\n",
    "* [Quickstart notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "* [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)\n",
    "* [Tensorflow model tuning with Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-tensorflow)\n",
    "\n",
    "---\n",
    "<sub><span id=\"azureml-search\">a. To use AzureML, you will need an Azure subscription.</span><br>\n",
    "<span id=\"azure-subscription\">b. When you web-search \"Azure Machine Learning\", you will most likely to see mixed results of Azure Machine Learning (AzureML) and Azure Machine Learning **Studio**. Please note they are different services where AzureML's focuses are on ML model management, tracking and hyperparameter tuning, while the [ML Studio](https://studio.azureml.net/)'s is to provide a high-level tool for 'easy-to-use' experience of ML designing and experimentation based on GUI.</span></sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version: 1.0.18\n",
      "Tensorflow Version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import azureml as aml\n",
    "import azureml.widgets as widgets\n",
    "import azureml.train.hyperdrive as hd\n",
    "\n",
    "from reco_utils.dataset.pandas_df_utils import user_item_pairs\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "import reco_utils.evaluation.python_evaluation as evaluator\n",
    "\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "\n",
    "# Temp dir to cache temporal files while running this notebook\n",
    "tmp_dir = TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# AzureML workspace information. Set them to create a workspace.\n",
    "SUBSCRIPTION_ID = None  #'<subscription-id>'\n",
    "RESOURCE_GROUP = None   #'<resource-group>'\n",
    "WORKSPACE_NAME = None   #'<workspace-name>'\n",
    "LOCATION = None         #'<region-to-deploy-the-workspace>'\n",
    "\n",
    "# Your Azure tenant id for authentication\n",
    "TENANT_ID = None        #'<tenant-id>'\n",
    "\n",
    "# Remote compute (cluster) configuration. If you want to save the cost more, set these to small.\n",
    "VM_SIZE = 'STANDARD_NC6'\n",
    "VM_PRIORITY = 'lowpriority'\n",
    "# Cluster nodes\n",
    "MIN_NODES = 0\n",
    "MAX_NODES = 8\n",
    "# Hyperdrive experimentation configuration\n",
    "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search the best hyperparameters. \n",
    "MAX_CONCURRENT_RUNS = 8\n",
    "\n",
    "# Recommend top k items\n",
    "TOP_K = 10\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "EPOCHS = 50\n",
    "# Metrics to track\n",
    "RANKING_METRICS = {\n",
    "    evaluator.ndcg_at_k.__name__: evaluator.ndcg_at_k,\n",
    "    evaluator.precision_at_k.__name__: evaluator.precision_at_k,\n",
    "}\n",
    "RATING_METRICS = {\n",
    "    evaluator.rmse.__name__: evaluator.rmse,\n",
    "    evaluator.mae.__name__: evaluator.mae,\n",
    "}\n",
    "PRIMARY_METRIC = evaluator.rmse.__name__\n",
    "# Data column names\n",
    "USER_COL = 'UserId'\n",
    "ITEM_COL = 'MovieId'\n",
    "RATING_COL = 'Rating'\n",
    "ITEM_FEAT_COL = 'Genres'\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create and Configure AzureML Workspace\n",
    "**AzureML workspace** is a foundational block in the cloud that you use to experiment, train, and deploy machine learning models via AzureML service. In this notebook, we 1) create a workspace from [**Azure portal**](https://portal.azure.com) and 2) configure from this notebook.\n",
    "\n",
    "You can find more details about the setup and configure processes from the following links:\n",
    "* [Quickstart with Azure portal](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started)\n",
    "* [Quickstart with Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "\n",
    "There are several ways to create an Azure Machine Learning service workspace.\n",
    "* Option 1: Use Azure portal\n",
    "    1. Sign in to the [Azure portal](https://portal.azure.com) by using the credentials for the Azure subscription you use.\n",
    "    2. Select **Create a resource** menu, search for **Machine Learning service workspace**, and select **Create** button.\n",
    "    3. In the **ML service workspace** pane, configure your workspace with entering the *workspace name* and *resource group* (or **create new** resource group if you don't have one already), and select **Create**. It can take a few moments to create the workspace.\n",
    "    4. Download **config.json** file from the portal's AzureML workspace page and place it to `<this-notebook-folder>/aml_config/config.json`\n",
    "* Option 2: Use [AzureML SDK](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py#workspace) - Run following cell\n",
    "    * To find the full list of supported region, use Azure CLI from [your machine](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest) or [cloud shell](https://azure.microsoft.com/en-us/features/cloud-shell/) to run: `az account list-locations`\n",
    "    * To locate your tenant id, use Azure CLI to run: `az account show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\jumin\\git\\reco\\notebooks\\04_model_select_and_optimize\\aml_config\\config.json\n"
     ]
    }
   ],
   "source": [
    "if TENANT_ID:\n",
    "    auth = aml.core.authentication.InteractiveLoginAuthentication(\n",
    "        tenant_id=TENANT_ID\n",
    "    )\n",
    "else:\n",
    "    auth = None  \n",
    "\n",
    "# If you are creating a new workspace\n",
    "if SUBSCRIPTION_ID and RESOURCE_GROUP and WORKSPACE_NAME and LOCATION:\n",
    "    ws = aml.core.Workspace.create(\n",
    "        name=WORKSPACE_NAME,\n",
    "        subscription_id=SUBSCRIPTION_ID,\n",
    "        resource_group=RESOURCE_GROUP,\n",
    "        create_resource_group=True,\n",
    "        location=LOCATION,\n",
    "        auth=auth,\n",
    "    )\n",
    "    ws.write_config()\n",
    "# If you are using an already-configured workspace (config.json)\n",
    "else:\n",
    "    ws = aml.core.Workspace.from_config(auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify your workspace, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureML workspace name:  junminaml\n"
     ]
    }
   ],
   "source": [
    "print(\"AzureML workspace name: \", ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Remote Compute Target\n",
    "\n",
    "We create a GPU cluster as our **remote compute target**. If a cluster with the same name is already exist in your workspace, the script will load it instead. You can see [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a compute target on different locations.\n",
    "\n",
    "This notebook selects **STANDARD_NC6** virtual machine (VM) and sets it's priority as *lowpriority* to save the cost.\n",
    "\n",
    "Size | vCPU | Memory (GiB) | Temp storage (SSD, GiB) | GPU | GPU memory (GiB) | Max data disks | Max NICs\n",
    "---|---|---|---|---|---|---|---\n",
    "Standard_NC6 | <div align=\"center\">6</div> | <div align=\"center\">56</div> | <div align=\"center\">340</div> | <div align=\"center\">1</div> | <div align=\"center\">8</div> | <div align=\"center\">24</div> | <div align=\"center\">1</div>\n",
    "\n",
    "\n",
    "For more information about Azure virtual machine sizes, see [here](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-06-18T21:39:43.968000+00:00', 'errors': None, 'creationTime': '2019-06-18T21:09:39.101231+00:00', 'modifiedTime': '2019-06-18T21:09:55.347615+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 8, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "CLUSTER_NAME = 'gpu-cluster-nc6'\n",
    "\n",
    "try:\n",
    "    compute_target = aml.core.compute.ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "    print(\"Found existing compute target\")\n",
    "except aml.core.compute_target.ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = aml.core.compute.AmlCompute.provisioning_configuration(\n",
    "        vm_size=VM_SIZE,\n",
    "        vm_priority=VM_PRIORITY,\n",
    "        min_nodes=MIN_NODES,\n",
    "        max_nodes=MAX_NODES\n",
    "    )\n",
    "    # create the cluster\n",
    "    compute_target = aml.core.compute.ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "For demonstration purpose, we use 100k MovieLens dataset. First, download the data and convert the format (multi-hot encode *genres*) to make it work for our model. More details about this step is described in our [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4.81k/4.81k [00:00<00:00, 5.17kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  MovieId  Rating                                             Genres\n",
       "0     196      242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1      63      242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2     226      242     5.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3     154      242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4     306      242     5.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[USER_COL, ITEM_COL, RATING_COL],\n",
    "    genres_col=ITEM_FEAT_COL\n",
    ")\n",
    "\n",
    "# Encode 'genres' into int array (multi-hot representation) to use as item features\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "data[ITEM_FEAT_COL] = genres_encoder.fit_transform(\n",
    "    data[ITEM_FEAT_COL].apply(lambda s: s.split(\"|\"))\n",
    ").tolist()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into train, validation, and test sets. The train and validation sets will be used for hyperparameter tuning, and the test set will be used for the final evaluation of the model after we import the best model from AzureML workspace.\n",
    "\n",
    "Here, we don't use multiple-split directly by passing `ratio=[0.56, 0.19, 0.25]`. Instead, we first split the data into train and test sets with the same `seed` we've been using in other notebooks to make the train set identical across them. Then, we further split the train set into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training / validation / test samples = 56250 / 18750 / 25000\n"
     ]
    }
   ],
   "source": [
    "# Use the same seed to make the train and test sets identical across other notebooks in the repo.\n",
    "train, test = python_random_split(data, ratio=0.75, seed=SEED)\n",
    "# Further split the train set into train and validation set.\n",
    "train, valid = python_random_split(train, seed=SEED)\n",
    "\n",
    "print(\"Number of training / validation / test samples = {} / {} / {}\".format(len(train), len(valid), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, upload the train and validation sets to the AzureML workspace. Our Hyperdrivce experiment will use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading C:\\Users\\jumin\\AppData\\Local\\Temp\\tmptwrt0vvi\\aml_data\\movielens_100k_train.pkl\n",
      "Uploading C:\\Users\\jumin\\AppData\\Local\\Temp\\tmptwrt0vvi\\aml_data\\movielens_100k_valid.pkl\n",
      "Uploaded C:\\Users\\jumin\\AppData\\Local\\Temp\\tmptwrt0vvi\\aml_data\\movielens_100k_valid.pkl, 1 files out of an estimated total of 2\n",
      "Uploaded C:\\Users\\jumin\\AppData\\Local\\Temp\\tmptwrt0vvi\\aml_data\\movielens_100k_train.pkl, 2 files out of an estimated total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_fbbe27f947ff4090aae61ca4d0e5968c"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(tmp_dir.name, 'aml_data') \n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n",
    "VALID_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_valid.pkl\"\n",
    "valid.to_pickle(os.path.join(DATA_DIR, VALID_FILE_NAME))\n",
    "\n",
    "# Note, all the files under DATA_DIR will be uploaded to the data store\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(\n",
    "    src_dir=DATA_DIR,\n",
    "    target_path='data',\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare Training Scripts\n",
    "Next step is to prepare scripts that AzureML Hyperdrive will use to train and evaluate models with selected hyperparameters. We re-use our [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb) for that. To run the model notebook from the Hyperdrive Run, all we need is to prepare an [entry script](../../reco_utils/azureml/wide_deep.py) which parses the hyperparameter arguments, passes them to the notebook, and records the results of the notebook to AzureML Run logs by using `papermill`. Hyperdrive uses the logs to track the performance of each hyperparameter-set and finds the best performed one.  \n",
    "\n",
    "Here is a code snippet from the entry script:\n",
    "```\n",
    "...\n",
    "from azureml.core import Run\n",
    "run = Run.get_context()\n",
    "...\n",
    "NOTEBOOK_NAME = os.path.join(\n",
    "    \"notebooks\",\n",
    "    \"00_quick_start\",\n",
    "    \"wide_deep_movielens.ipynb\"\n",
    ")\n",
    "...\n",
    "parser = argparse.ArgumentParser()\n",
    "...\n",
    "parser.add_argument('--dnn-optimizer', type=str, dest='dnn_optimizer', ...\n",
    "parser.add_argument('--dnn-optimizer-lr', type=float, dest='dnn_optimizer_lr', ...\n",
    "...\n",
    "pm.execute_notebook(\n",
    "    NOTEBOOK_NAME,\n",
    "    OUTPUT_NOTEBOOK,\n",
    "    parameters=params,\n",
    "    kernel_name='python3',\n",
    ")\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all the necessary scripts which will be loaded to our Hyperdrive Experiment Run\n",
    "SCRIPT_DIR = os.path.join(tmp_dir.name, 'aml_script')\n",
    "\n",
    "# Copy scripts to SCRIPT_DIR temporarly\n",
    "shutil.copytree(os.path.join('..', '..', 'reco_utils'), os.path.join(SCRIPT_DIR, 'reco_utils'))\n",
    "\n",
    "# We re-use our model notebook for training and testing models.\n",
    "model_notebook_dir = os.path.join('notebooks', '00_quick_start')\n",
    "dest_model_notebook_dir = os.path.join(SCRIPT_DIR, model_notebook_dir)\n",
    "os.makedirs(dest_model_notebook_dir , exist_ok=True)\n",
    "shutil.copy(\n",
    "    os.path.join('..', '..', model_notebook_dir, 'wide_deep_movielens.ipynb'),\n",
    "    dest_model_notebook_dir\n",
    ")\n",
    "\n",
    "# This is our entry script for Hyperdrive Run\n",
    "ENTRY_SCRIPT_NAME = 'reco_utils/azureml/wide_deep.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup and Run Hyperdrive Experiment\n",
    "[Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) create a machine learning Experiment [Run](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run?view=azure-ml-py) on the workspace and utilizes child-runs to search the best set of hyperparameters.\n",
    "\n",
    "#### 5.1 Create Experiment \n",
    "[Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py) is the main entry point into experimenting with AzureML. To create new Experiment or get the existing one, we pass our experimentation name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment to track the runs in the workspace\n",
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_wide_deep_model\"\n",
    "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Define Search Space \n",
    "Now we define the search space of hyperparameters. For example, if you want to test different batch sizes of {64, 128, 256}, you can use `azureml.train.hyperdrive.choice(64, 128, 256)`. To search from a continuous space, use `uniform(start, end)`. For more options, see [Hyperdrive parameter expressions](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py).\n",
    "In this notebook, we fix model type as `wide_deep` and the number of epochs to 50.\n",
    "\n",
    "In the search space, we set different linear and DNN optimizers, structures, learning rates and regularization rates. Details about the hyperparameters can be found from our [Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb).\n",
    "\n",
    "Hyperdrive provides three different parameter sampling methods: `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each method can be found from [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the Bayesian sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script parameters. New AzureML API only accepts string values.\n",
    "script_params = {\n",
    "    '--datastore': ds.as_mount(),\n",
    "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n",
    "    '--test-datapath': \"data/\" + VALID_FILE_NAME,\n",
    "    '--top-k': str(TOP_K),\n",
    "    '--user-col': USER_COL,\n",
    "    '--item-col': ITEM_COL,\n",
    "    '--item-feat-col': ITEM_FEAT_COL,\n",
    "    '--rating-col': RATING_COL,\n",
    "    '--ranking-metrics': RANKING_METRICS,\n",
    "    '--rating-metrics': RATING_METRICS,\n",
    "    '--epochs': str(EPOCHS),\n",
    "    '--model-type': 'wide_deep'\n",
    "}\n",
    "\n",
    "# Hyperparameter search space\n",
    "params = {\n",
    "    '--batch-size': hd.choice(64, 128, 256),\n",
    "    # Linear model hyperparameters\n",
    "    '--linear-optimizer': hd.choice('Ftrl'),  # 'SGD' and 'Momentum' easily got exploded loss in regression problems.\n",
    "    '--linear-optimizer-lr': hd.uniform(0.0001, 0.1),\n",
    "    '--linear-l1-reg': hd.uniform(0.0, 0.1),\n",
    "    # Deep model hyperparameters\n",
    "    '--dnn-optimizer': hd.choice('Adagrad', 'Adam'),\n",
    "    '--dnn-optimizer-lr': hd.uniform(0.0001, 0.1),\n",
    "    '--dnn-user-embedding-dim': hd.choice(4, 8, 16, 32, 64),\n",
    "    '--dnn-item-embedding-dim': hd.choice(4, 8, 16, 32, 64),\n",
    "    '--dnn-hidden-layer-1': hd.choice(0, 32, 64, 128, 256, 512, 1024),  # 0: not using this layer\n",
    "    '--dnn-hidden-layer-2': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n",
    "    '--dnn-hidden-layer-3': hd.choice(0, 32, 64, 128, 256, 512, 1024),\n",
    "    '--dnn-hidden-layer-4': hd.choice(32, 64, 128, 256, 512, 1024),\n",
    "    '--dnn-batch-norm': hd.choice(0, 1),\n",
    "    '--dnn-dropout': hd.choice(0.0, 0.1, 0.2, 0.3, 0.4)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AzureML Estimator** is the building block for training. An Estimator encapsulates the training code and parameters, the compute resources and runtime environment for a particular training scenario (Note, this is not TensorFlow's Estimator)\n",
    "\n",
    "We create one for our experimentation with the dependencies our model requires as follows:\n",
    "```\n",
    "conda_packages=['pandas', 'scikit-learn'],\n",
    "pip_packages=['ipykernel', 'papermill', 'tensorflow-gpu==1.12']\n",
    "```\n",
    "\n",
    "To the Hyperdrive Run Config, we set our primary metric name and the goal (our hyperparameter search criteria), hyperparameter sampling method, and number of total child-runs. The bigger the search space, the more number of runs we will need for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = aml.train.estimator.Estimator(\n",
    "    source_directory=SCRIPT_DIR,\n",
    "    entry_script=ENTRY_SCRIPT_NAME,\n",
    "    script_params=script_params,\n",
    "    compute_target=compute_target,\n",
    "    use_gpu=True,\n",
    "    conda_packages=['pandas', 'scikit-learn', 'numba'],\n",
    "    pip_packages=['ipykernel', 'papermill==0.18.2', 'tensorflow-gpu==1.12']\n",
    ")\n",
    "\n",
    "hd_run_config = hd.HyperDriveRunConfig(\n",
    "    estimator=est, \n",
    "    hyperparameter_sampling=hd.BayesianParameterSampling(params),\n",
    "    primary_metric_name=PRIMARY_METRIC,\n",
    "    primary_metric_goal=hd.PrimaryMetricGoal.MINIMIZE, \n",
    "    max_total_runs=MAX_TOTAL_RUNS,\n",
    "    max_concurrent_runs=MAX_CONCURRENT_RUNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Run Experiment\n",
    "\n",
    "Now we submit the Run to our experiment. You can see the experiment progress from this notebook by using `azureml.widgets.RunDetails(hd_run).show()` or check from the Azure portal with the url link you can get by running `hd_run.get_portal_url()`.\n",
    "\n",
    "<img src=\"https://recodatasets.blob.core.windows.net/images/aml_0.png?sanitize=true\" width=\"600\"/>\n",
    "<img src=\"https://recodatasets.blob.core.windows.net/images/aml_1.png?sanitize=true\" width=\"600\"/>\n",
    "<center><i>AzureML Hyperdrive Widget</i></center>\n",
    "\n",
    "To load an existing Hyperdrive Run instead of start new one, use `hd_run = hd.HyperDriveRun(exp, <user-run-id>, hyperdrive_run_config=hd_run_config)`. You also can cancel the Run with `hd_run.cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a01bf0272a349019807126cc362fd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hd_run = exp.submit(config=hd_run_config)\n",
    "widgets.RunDetails(hd_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the child-runs are finished, we can get the best run and the metrics.\n",
    "> Note, if you run Hyperdrive experiment again, you will see the best metrics and corresponding hyperparameters are not the same. It is because of 1) the random initialization of the model and 2) Hyperdrive sampling (when you use RandomSampling). You will get different results as well if you use different training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and printout metrics\n",
    "best_run = hd_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Best Run Id: movielens_100k_wide_deep_model_1560996258088_33\n",
      "\n",
      "* Best hyperparameters:\n",
      "Model type = wide_deep\n",
      "Batch size = 64.0\n",
      "Linear optimizer = Ftrl\n",
      "\tLearning rate = 0.0001\n",
      "\tL1 regularization = 0.0390\n",
      "DNN optimizer = Adagrad\n",
      "\tUser embedding dimension = 64.0\n",
      "\tItem embedding dimension = 32.0\n",
      "\tHidden units = [32.0]\n",
      "\tLearning rate = 0.1000\n",
      "\tDropout rate = 0.4000\n",
      "\tBatch normalization = 0.0\n",
      "\n",
      "* Performance metrics:\n",
      "\tndcg_at_k (top-10) = 0.0033\n",
      "\tprecision_at_k (top-10) = 0.0039\n",
      "\trmse = 0.9483\n",
      "\tmae = 0.7483\n"
     ]
    }
   ],
   "source": [
    "print(\"* Best Run Id:\", best_run.id)\n",
    "print(\"\\n* Best hyperparameters:\")\n",
    "print(\"Model type =\", best_run_metrics['MODEL_TYPE'])\n",
    "print(\"Batch size =\", best_run_metrics['BATCH_SIZE'])\n",
    "print(\"Linear optimizer =\", best_run_metrics['LINEAR_OPTIMIZER'])\n",
    "print(\"\\tLearning rate = {0:.4f}\".format(best_run_metrics['LINEAR_OPTIMIZER_LR']))\n",
    "print(\"\\tL1 regularization = {0:.4f}\".format(best_run_metrics['LINEAR_L1_REG']))\n",
    "print(\"DNN optimizer =\", best_run_metrics['DNN_OPTIMIZER'])\n",
    "print(\"\\tUser embedding dimension =\", best_run_metrics['DNN_USER_DIM'])\n",
    "print(\"\\tItem embedding dimension =\", best_run_metrics['DNN_ITEM_DIM'])\n",
    "hidden_units = []\n",
    "for i in range(1, 5):\n",
    "    hidden_nodes = best_run_metrics['DNN_HIDDEN_LAYER_{}'.format(i)]\n",
    "    if hidden_nodes > 0:\n",
    "        hidden_units.append(hidden_nodes)\n",
    "print(\"\\tHidden units =\", hidden_units)\n",
    "print(\"\\tLearning rate = {0:.4f}\".format(best_run_metrics['DNN_OPTIMIZER_LR']))\n",
    "print(\"\\tDropout rate = {0:.4f}\".format(best_run_metrics['DNN_DROPOUT']))\n",
    "print(\"\\tBatch normalization =\", best_run_metrics['DNN_BATCH_NORM'])\n",
    "# Metrics evaluated on validation set\n",
    "print(\"\\n* Performance metrics:\")\n",
    "for m in RANKING_METRICS:\n",
    "    print(\"\\t{0} (top-{1}) = {2:.4f}\".format(m, TOP_K, best_run_metrics[m]))\n",
    "for m in RATING_METRICS:\n",
    "    print(\"\\t{0} = {1:.4f}\".format(m, best_run_metrics[m]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Import and Test\n",
    "\n",
    "[Wide-Deep Quickstart notebook](../00_quick_start/wide_deep_movielens.ipynb), which we've used in our Hyperdrive Experiment, exports the trained model to the output folder (the output path is recorded at `best_run_metrics['saved_model_dir']`). We can download a model from the best run and test it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/model/1560998954/\n",
      "Downloading outputs/model/1560998954/saved_model.pb..\n",
      "Downloading outputs/model/1560998954/variables/variables.data-00000-of-00002..\n",
      "Downloading outputs/model/1560998954/variables/variables.data-00001-of-00002..\n",
      "Downloading outputs/model/1560998954/variables/variables.index..\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = os.path.join(tmp_dir.name, 'aml_model')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Cleanup dir name as dir name are being saved as \"b'./outputs/...'\"\n",
    "model_file_dir = best_run_metrics['saved_model_dir'][4:-1] + '/'\n",
    "print(model_file_dir)\n",
    "\n",
    "for f in best_run.get_file_names():\n",
    "    if f.startswith(model_file_dir):\n",
    "        output_file_path = os.path.join(MODEL_DIR, f.split(model_file_dir)[1])\n",
    "        print(\"Downloading {}..\".format(f))\n",
    "        best_run.download_file(name=f, output_file_path=output_file_path)\n",
    "    \n",
    "saved_model = tf.contrib.estimator.SavedModelEstimator(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {\n",
    "    'col_user': USER_COL,\n",
    "    'col_item': ITEM_COL,\n",
    "    'col_rating': RATING_COL,\n",
    "    'col_prediction': 'prediction'\n",
    "}\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Prediction input function for TensorFlow SavedModel\n",
    "def predict_input_fn(df):\n",
    "    def input_fn():\n",
    "        examples = [None] * len(df)\n",
    "        for index, test_sample in df.iterrows():\n",
    "            example = tf.train.Example()\n",
    "\n",
    "            example.features.feature[USER_COL].int64_list.value.extend([test_sample[USER_COL]])\n",
    "            example.features.feature[ITEM_COL].int64_list.value.extend([test_sample[ITEM_COL]])\n",
    "            example.features.feature[ITEM_FEAT_COL].float_list.value.extend(test_sample[ITEM_FEAT_COL])\n",
    "\n",
    "            examples[index] = example.SerializeToString()\n",
    "        return {'inputs': tf.constant(examples)}\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    25000.000000\n",
      "mean         3.481185\n",
      "std          0.681335\n",
      "min         -0.697048\n",
      "25%          3.144000\n",
      "50%          3.544610\n",
      "75%          3.945009\n",
      "max          5.654730\n",
      "Name: prediction, dtype: float64 \n",
      "\n",
      "rmse = 0.9453778543937227\n",
      "mae = 0.7472773903702199\n"
     ]
    }
   ],
   "source": [
    "# Rating prediction set\n",
    "X_test = test.drop(RATING_COL, axis=1)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rating prediction\n",
    "predictions = list(itertools.islice(\n",
    "    saved_model.predict(predict_input_fn(X_test)),\n",
    "    len(X_test)\n",
    "))\n",
    "\n",
    "prediction_df = X_test.copy()\n",
    "prediction_df['prediction'] = [p['outputs'][0] for p in predictions]\n",
    "print(prediction_df['prediction'].describe(), \"\\n\")\n",
    "for m in RATING_METRICS:\n",
    "    fn = getattr(reco_utils.evaluation.python_evaluation, m)\n",
    "    result = fn(test, prediction_df, **cols)\n",
    "    print(m, \"=\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique items\n",
    "if ITEM_FEAT_COL is None:\n",
    "    items = data.drop_duplicates(ITEM_COL)[[ITEM_COL]].reset_index(drop=True)\n",
    "else:\n",
    "    items = data.drop_duplicates(ITEM_COL)[[ITEM_COL, ITEM_FEAT_COL]].reset_index(drop=True)\n",
    "# Unique users\n",
    "users = data.drop_duplicates(USER_COL)[[USER_COL]].reset_index(drop=True)\n",
    "\n",
    "# Ranking prediction set\n",
    "ranking_pool = user_item_pairs(\n",
    "    user_df=users,\n",
    "    item_df=items,\n",
    "    user_col=USER_COL,\n",
    "    item_col=ITEM_COL,\n",
    "    user_item_filter_df=pd.concat([train, valid]),  # remove seen items\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg_at_k = 0.00437727611893178\n",
      "precision_at_k = 0.0042417815482502655\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "# To prevent creating a tensor proto whose content is larger than 2GB (which will raise an error),\n",
    "# divide ranking_pool into 10 chunks, predict each, and concat back. \n",
    "for pool in np.array_split(ranking_pool, 10):\n",
    "    pool.reset_index(drop=True, inplace=True)\n",
    "    # Rating prediction\n",
    "    pred = list(itertools.islice(\n",
    "        saved_model.predict(predict_input_fn(pool)),\n",
    "        len(pool)\n",
    "    ))\n",
    "    predictions.extend([p['outputs'][0] for p in pred])\n",
    "    \n",
    "ranking_pool['prediction'] = predictions\n",
    "\n",
    "for m in RANKING_METRICS:\n",
    "    fn = getattr(reco_utils.evaluation.python_evaluation, m)\n",
    "    result = fn(test, ranking_pool, **{**cols, 'k': TOP_K})\n",
    "    print(m, \"=\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide-and-Deep Baseline Comparison\n",
    "To see if Hyperdrive found good hyperparameters, we simply compare with the model with known hyperparameters from [TensorFlow's wide-deep learning example](https://github.com/tensorflow/models/blob/master/official/wide_deep/movielens_main.py) which uses only the DNN part from the wide-and-deep model for MovieLens data.\n",
    "\n",
    "> Note, this is not 'apples to apples' comparison. For example, TensorFlow's movielens example uses *rating-timestamp* as a numeric feature, but we did not use that here because we think the timestamps are not relevant to the movies' ratings. This comparison is more like to show how Hyperdrive can help to find comparable hyperparameters without requiring exhaustive efforts in going over a huge search-space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7b9dd464f34ab4844a065085abd7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluation of Wide-and-Deep model took 197.87058877944946 secs.\n",
      "ndcg_at_k = 0.01261974545470797\n",
      "precision_at_k = 0.014316012725344647\n",
      "rmse = 1.0120205005713365\n",
      "mae = 0.802690488152504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/reco_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: Function read_notebook is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.read_notebook` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_NOTEBOOK = os.path.join(tmp_dir.name, \"output.ipynb\")\n",
    "OUTPUT_MODEL_DIR = os.path.join(tmp_dir.name, \"known_hyperparam_model_checkpoints\")\n",
    "\n",
    "params = {\n",
    "    'MOVIELENS_DATA_SIZE': MOVIELENS_DATA_SIZE,\n",
    "    'TOP_K': TOP_K,\n",
    "    'MODEL_TYPE': 'deep',\n",
    "    'EPOCHS': EPOCHS,\n",
    "    'BATCH_SIZE': 256,\n",
    "    'DNN_OPTIMIZER': 'Adam',\n",
    "    'DNN_OPTIMIZER_LR': 0.001,\n",
    "    'DNN_HIDDEN_LAYER_1': 256,\n",
    "    'DNN_HIDDEN_LAYER_2': 256,\n",
    "    'DNN_HIDDEN_LAYER_3': 256,\n",
    "    'DNN_HIDDEN_LAYER_4': 128,\n",
    "    'DNN_USER_DIM': 16,\n",
    "    'DNN_ITEM_DIM': 64,\n",
    "    'DNN_DROPOUT': 0.3,\n",
    "    'DNN_BATCH_NORM': 0,\n",
    "    'MODEL_DIR': OUTPUT_MODEL_DIR,\n",
    "    'EVALUATE_WHILE_TRAINING': False,\n",
    "    'EXPORT_DIR_BASE': OUTPUT_MODEL_DIR,\n",
    "    'RANKING_METRICS': RANKING_METRICS,\n",
    "    'RATING_METRICS': RATING_METRICS,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "pm.execute_notebook(\n",
    "    \"../00_quick_start/wide_deep_movielens.ipynb\",\n",
    "    OUTPUT_NOTEBOOK,\n",
    "    parameters=params,\n",
    "    kernel_name='python3'\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Training and evaluation of Wide-and-Deep model took\", end_time-start_time, \"secs.\")\n",
    "\n",
    "nb = pm.read_notebook(OUTPUT_NOTEBOOK)\n",
    "for m in RANKING_METRICS:\n",
    "    print(m, \"=\", nb.data[m])\n",
    "for m in RATING_METRICS:\n",
    "    print(m, \"=\", nb.data[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remark\n",
    "We showed how to tune hyperparameters by utilizing Azure Machine Learning service. Complex and powerful models like Wide-and-Deep model often have many number of hyperparameters that affect on the recommendation accuracy, and it is not practical to tune the model without using a GPU cluster. For example, a training and evaluation of a model took around 3 minutes on 100k MovieLens data on a single *Standard NC6* VM as we tested from the [above cell](#Wide-and-Deep-Baseline-Comparison). When we used 1M MovieLens, it took about 47 minutes. If we want to investigate through 100 different combinations of hyperparameters **manually**, it will take **78 hours** on the VM and we may still wonder if we had tested good candidates of hyperparameters. With AzureML, as we shown in this notebook, we can easily setup different size of GPU cluster fits to our problem and utilize Bayesian sampling to navigate through the huge search space efficiently, and tweak the experiment with different criteria and algorithms for further research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "reco_gpu",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
