{"cells":[{"cell_type":"markdown","source":["<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n\n<i>Licensed under the MIT License.</i>"],"metadata":{}},{"cell_type":"markdown","source":["# Vowpal Wabbit Deep Dive"],"metadata":{}},{"cell_type":"markdown","source":["<center>\n<img src=\"https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/logo_assets/vowpal-wabbits-github-logo.png?raw=true\" height=\"30%\" width=\"30%\" alt=\"Vowpal Wabbit\">\n</center>\n\n[Vowpal Wabbit](https://github.com/VowpalWabbit/vowpal_wabbit) は、レコメンデーション用途に関連するいくつかのアルゴリズムを実装する高速オンライン機械学習ライブラリです。\n\nVowpal Wabbit(VW)の主な利点は、トレーニングは通常、確率的勾配降下または類似手法を使用してオンラインで行われ、非常に大きなデータセットにうまくスケーリングできることです。さらに、非常に高速に実行するように最適化されており、非常に大規模なデータセットの分散トレーニング シナリオをサポートできます。\n\nVW は、大きすぎてメモリには収まらないが、単一のノードのディスクであれば格納できるようなデータサイズの問題に最適です。分散トレーニングはノードの追加のセットアップと構成で可能です。VWが適切に扱う問題の種類は、主に機械学習の教師あり分類の領域(線形回帰、ロジスティック回帰、マルチクラス分類、サポートベクターマシン、シンプルニューラルネット)に分類されます。また、行列因子化アプローチと潜在ディリクレット割り当て、およびその他のいくつかのアルゴリズムもサポートしています(詳細については[wiki](https://github.com/VowpalWabbit/vowpal_wabbit/wiki)を参照してください)。\n\n一般的な良い展開例は、ユーザーの広告を掲載するオークションがミリ秒単位で決定されるリアルタイム入札シナリオです。ユーザーとアイテムに関するフィーチャ情報を抽出してモデルに渡し、クリック (またはその他の反応) の可能性を短い時間で予測する必要があります。また、ユーザーとコンテキストのフィーチャーが常に変化している場合 (例えば、ユーザーの利用ブラウザや現地時間)、可能なすべての入力の組み合わせを事前にスコア付けすることは不可能な場合があります。VWは、さまざまなアルゴリズムをオフラインで探索し、大量の履歴データで高精度なモデルをトレーニングし、リアルタイムで迅速な予測を生成できるように、モデルを本番環境に展開するプラットフォームとして価値を提供します。もちろん、これはVWを展開できる唯一の方法ではなく、モデルが絶えず更新されている完全にオンラインな環境で使用したり、アクティブラーニングアプローチを使用したり、事前スコアリングモードで完全にオフラインで作業することも可能です。"],"metadata":{}},{"cell_type":"markdown","source":["<h3>Recommendations 用の Vowpal Wabbit</h3>\nこのノートブックでは、VW ライブラリを使用して [MovieLens](https://grouplens.org/datasets/movielens/) データセットに関するレコメンデーションを生成する方法を示します。\n\nこのノートブックで VW がどのように使用されるかについて、以下の内容に注目してください。\n\nAzure Data Science Virtual Machine ([DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)) を使用すると VW はプリインストールされており、コマンド ラインから直接使用できます。DSVM を使用していない場合は、vw を自分でインストールする必要があります。\n\nまた、Python 環境内で VW を使用できるようにする Python バインディングや、SciKit-Learn Estimator API に準拠したラッパーもあります。ただし、Python バインディングは Boost と依存関係を持つ追加の Python パッケージとしてインストールする必要があるため、VW の実行を簡潔にするために、モデルのコマンド ライン実行から動作を模倣するサブプロセス呼び出しを介して行われます。\n\nVWは特定の[入力フォーマット](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Input-format)を期待しており、このノートブックの to_vw() は、標準的な MovieLens データセットを対応するデータ形式に変換する便利な機能です。その後、データファイルはディスクに書き込まれ、トレーニングのためにVWに渡されます。\n\n以下の例は、異なるアプローチのパフォーマンス上の利点を示さない VW の機能的能力を示すものです。[コマンドライン オプション](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-Line-Arguments)を使用して調整できるVWモデルのパフォーマンスに大きな影響を与えるハイパーパラメータ(学習率や正規化用語など)がいくつかあります。アプローチを適切に比較するには、関連するデータセットについて学習し、調整すると便利でしょう。"],"metadata":{}},{"cell_type":"markdown","source":["# 0. グローバル セットアップ"],"metadata":{}},{"cell_type":"code","source":["%sh # Databricks での Vowpal Wabbit のインストール例\napt-get update\napt-get -y install libboost-all-dev\napt-get install -y cmake\ncd /databricks/driver\ngit clone git://github.com/VowpalWabbit/vowpal_wabbit.git\nmkdir vowpal_wabbit/build\ncd vowpal_wabbit/build\ncmake .. -DBUILD_PYTHON=ON -DSTATIC_LINK_VW=ON\nmake -j\nln -s /databricks/driver/vowpal_wabbit/build/vowpalwabbit/vw /usr/local/bin/vw\n/usr/local/bin/vw -h"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hit:1 https://cran.rstudio.com/bin/linux/ubuntu xenial-cran35// InRelease\nIgn:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\nIgn:3 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\nHit:4 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release\nHit:5 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\nHit:7 http://security.ubuntu.com/ubuntu xenial-security InRelease\nHit:8 http://archive.ubuntu.com/ubuntu xenial InRelease\nHit:10 http://archive.ubuntu.com/ubuntu xenial-updates InRelease\nHit:11 http://archive.ubuntu.com/ubuntu xenial-backports InRelease\nReading package lists...\nReading package lists...\nBuilding dependency tree...\nReading state information...\nlibboost-all-dev is already the newest version (1.58.0.1ubuntu1).\nThe following package was automatically installed and is no longer required:\n  libgnutls-openssl27\nUse &#39;sudo apt autoremove&#39; to remove it.\n0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\nReading package lists...\nBuilding dependency tree...\nReading state information...\ncmake is already the newest version (3.5.1-1ubuntu3).\nThe following package was automatically installed and is no longer required:\n  libgnutls-openssl27\nUse &#39;sudo apt autoremove&#39; to remove it.\n0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\nfatal: destination path &#39;vowpal_wabbit&#39; already exists and is not an empty directory.\nmkdir: cannot create directory ‘vowpal_wabbit/build’: File exists\ncmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\ncmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\ncmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\ncmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n-- VowpalWabbit Version: 8.7.0\n-- Git Version: b4a54fc\n-- Number of processors: 12\n-- Boost version: 1.58.0\n-- Found the following Boost libraries:\n--   program_options\n--   system\n--   thread\n--   unit_test_framework\n--   chrono\n--   date_time\n--   atomic\n-- Submodule update\n-- help2man not found, please install it to generate manpages\nCMake Warning at python/CMakeLists.txt:3 (message):\n  No PY_VERSION specified, Python 2.7 will be used for VowpalWabbit Python\n  bindings\n\n\n-- Boost version: 1.58.0\n-- Found the following Boost libraries:\n--   system\n--   python\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /databricks/driver/vowpal_wabbit/build\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n[  3%] Built target spanning_tree\n[  6%] Built target allreduce\n[  7%] Built target active_interactor\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n[ 74%] Built target vw\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n[ 76%] Built target ezexample_predict\n[ 77%] Built target vw_c_wrapper\n[ 78%] Built target library_example\n[ 80%] Built target gd_mf_weights\n[ 84%] Built target test_search\n[ 84%] Built target vw-bin\n[ 84%] Built target search_generate\n[ 85%] Built target recommend\n[ 86%] Built target ezexample_train\n[ 88%] Built target ezexample_predict_threaded\n[ 89%] Linking CXX shared library pylibvw.so\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libssl.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/cmake: /databricks/python/lib/libcrypto.so.1.0.0: no version information available (required by /usr/lib/x86_64-linux-gnu/libcurl.so.4)\n/usr/bin/ld: /usr/lib/x86_64-linux-gnu/libpython2.7.a(abstract.o): relocation R_X86_64_32 against `.rodata.str1.8&#39; can not be used when making a shared object; recompile with -fPIC\n/usr/lib/x86_64-linux-gnu/libpython2.7.a: error adding symbols: Bad value\ncollect2: error: ld returned 1 exit status\npython/CMakeFiles/pylibvw.dir/build.make:101: recipe for target &#39;python/pylibvw.so&#39; failed\nmake[2]: *** [python/pylibvw.so] Error 1\nCMakeFiles/Makefile2:722: recipe for target &#39;python/CMakeFiles/pylibvw.dir/all&#39; failed\nmake[1]: *** [python/CMakeFiles/pylibvw.dir/all] Error 2\nmake[1]: *** Waiting for unfinished jobs....\n[ 99%] Built target vw-unit-test.out\nMakefile:138: recipe for target &#39;all&#39; failed\nmake: *** [all] Error 2\nln: failed to create symbolic link &#39;/usr/local/bin/vw&#39;: File exists\nNum weight bits = 18\nlearning rate = 0.5\ninitial_t = 0\npower_t = 0.5\nusing no cache\nReading datafile = \nnum sources = 1\ndriver:\n  --onethread           Disable parse thread\nVW options:\n  --ring_size arg (=256, ) size of example ring\n  --strict_parse           throw on malformed examples\nUpdate options:\n  -l [ --learning_rate ] arg Set learning rate\n  --power_t arg              t power value\n  --decay_learning_rate arg  Set Decay factor for learning_rate between passes\n  --initial_t arg            initial t value\n  --feature_mask arg         Use existing regressor to determine which \n                             parameters may be updated.  If no \n                             initial_regressor given, also used for initial \n                             weights.\nWeight options:\n  -i [ --initial_regressor ] arg  Initial regressor(s)\n  --initial_weight arg            Set all weights to an initial value of arg.\n\n*** WARNING: skipped 8219 bytes of output ***\n\n                        this mode.\nSearch options:\n  --search arg                          Use learning to search, \n                                        argument=maximum action id or 0 for LDF\n  --search_task arg                     the search task (use &#34;--search_task \n                                        list&#34; to get a list of available tasks)\n  --search_metatask arg                 the search metatask (use \n                                        &#34;--search_metatask list&#34; to get a list \n                                        of available metatasks)\n  --search_interpolation arg            at what level should interpolation \n                                        happen? [*data|policy]\n  --search_rollout arg                  how should rollouts be executed?       \n                                            [policy|oracle|*mix_per_state|mix_p\n                                        er_roll|none]\n  --search_rollin arg                   how should past trajectories be \n                                        generated? [policy|oracle|*mix_per_stat\n                                        e|mix_per_roll]\n  --search_passes_per_policy arg (=1, ) number of passes per policy (only valid\n                                        for search_interpolation=policy)\n  --search_beta arg (=0.5, )            interpolation rate for policies (only \n                                        valid for search_interpolation=policy)\n  --search_alpha arg (=1e-10, )         annealed beta = 1-(1-alpha)^t (only \n                                        valid for search_interpolation=data)\n  --search_total_nb_policies arg        if we are going to train the policies \n                                        through multiple separate calls to vw, \n                                        we need to specify this parameter and \n                                        tell vw how many policies are \n                                        eventually going to be trained\n  --search_trained_nb_policies arg      the number of trained policies in a \n                                        file\n  --search_allowed_transitions arg      read file of allowed transitions [def: \n                                        all transitions are allowed]\n  --search_subsample_time arg           instead of training at all timesteps, \n                                        use a subset. if value in (0,1), train \n                                        on a random v%. if v&gt;=1, train on \n                                        precisely v steps per example, if \n                                        v&lt;=-1, use active learning\n  --search_neighbor_features arg        copy features from neighboring lines. \n                                        argument looks like: &#39;-1:a,+2&#39; meaning \n                                        copy previous line namespace a and next\n                                        next line from namespace _unnamed_, \n                                        where &#39;,&#39; separates them\n  --search_rollout_num_steps arg        how many calls of &#34;loss&#34; before we stop\n                                        really predicting on rollouts and \n                                        switch to oracle (default means \n                                        &#34;infinite&#34;)\n  --search_history_length arg (=1, )    some tasks allow you to specify how \n                                        much history their depend on; specify \n                                        that here\n  --search_no_caching                   turn off the built-in caching ability \n                                        (makes things slower, but technically \n                                        more safe)\n  --search_xv                           train two separate policies, \n                                        alternating prediction/learning\n  --search_perturb_oracle arg (=0, )    perturb the oracle on rollin with this \n                                        probability\n  --search_linear_ordering              insist on generating examples in linear\n                                        order (def: hoopla permutation)\n  --search_active_verify arg            verify that active learning is doing \n                                        the right thing (arg = multiplier, \n                                        should be = cost_range * range_c)\n  --search_save_every_k_runs arg        save model every k runs\nExperience Replay:\n  --replay_c arg              use experience replay at a specified level \n                              [b=classification/regression, m=multiclass, \n                              c=cost sensitive] with specified buffer size\n  --replay_c_count arg (=1, ) how many times (in expectation) should each \n                              example be played (default: 1 = permuting)\nExplore evaluation:\n  --explore_eval        Evaluate explore_eval adf policies\n  --multiplier arg      Multiplier used to make all rejection sample \n                        probabilities &lt;= 1\nMake csoaa_ldf into Contextual Bandit:\n  --cbify_ldf           Convert csoaa_ldf into a contextual bandit problem\n  --loss0 arg (=0, )    loss for correct label\n  --loss1 arg (=1, )    loss for incorrect label\nMake Multiclass into Contextual Bandit:\n  --cbify arg           Convert multiclass on &lt;k&gt; classes into a contextual \n                        bandit problem\n  --cbify_cs            consume cost-sensitive classification examples instead \n                        of multiclass\n  --loss0 arg (=0, )    loss for correct label\n  --loss1 arg (=1, )    loss for incorrect label\nMake Multiclass into Warm-starting Contextual Bandit:\n  --warm_cb arg                        Convert multiclass on &lt;k&gt; classes into a\n                                       contextual bandit problem\n  --warm_cb_cs                         consume cost-sensitive classification \n                                       examples instead of multiclass\n  --loss0 arg (=0, )                   loss for correct label\n  --loss1 arg (=1, )                   loss for incorrect label\n  --warm_start arg (=0, )              number of training examples for warm \n                                       start phase\n  --epsilon arg                        epsilon-greedy exploration\n  --interaction arg (=4294967295, )    number of examples for the interactive \n                                       contextual bandit learning phase\n  --warm_start_update                  indicator of warm start updates\n  --interaction_update                 indicator of interaction updates\n  --corrupt_type_warm_start arg (=1, ) type of label corruption in the warm \n                                       start phase (1: uniformly at random, 2: \n                                       circular, 3: replacing with overwriting \n                                       label)\n  --corrupt_prob_warm_start arg (=0, ) probability of label corruption in the \n                                       warm start phase\n  --choices_lambda arg (=1, )          the number of candidate lambdas to \n                                       aggregate (lambda is the importance \n                                       weight parameter between the two \n                                       sources)\n  --lambda_scheme arg (=1, )           The scheme for generating candidate \n                                       lambda set (1: center lambda=0.5, 2: \n                                       center lambda=0.5, min lambda=0, max \n                                       lambda=1, 3: center lambda=epsilon/(1+ep\n                                       silon), 4: center lambda=epsilon/(1+epsi\n                                       lon), min lambda=0, max lambda=1); the \n                                       rest of candidate lambda values are \n                                       generated using a doubling scheme\n  --overwrite_label arg (=1, )         the label used by type 3 corruptions \n                                       (overwriting)\n  --sim_bandit                         simulate contextual bandit updates on \n                                       warm start examples\nEXPERIMENTAL: Conditional Contextual Bandit Exploration with Action Dependent Features:\n  --ccb_explore_adf     EXPERIMENTAL: Do Conditional Contextual Bandit learning\n                        with multiline action dependent features.\nCB Sample:\n  --cb_sample           Sample from CB pdf and swap top action.\nContextual Bandit Exploration with Action Dependent Features:\n  --cb_explore_adf          Online explore-exploit for a contextual bandit \n                            problem with multiline action dependent features\n  --first arg               tau-first exploration\n  --epsilon arg             epsilon-greedy exploration\n  --bag arg                 bagging-based exploration\n  --cover arg               Online cover based exploration\n  --psi arg (=1, )          disagreement parameter for cover\n  --nounif                  do not explore uniformly on zero-probability \n                            actions in cover\n  --softmax                 softmax exploration\n  --regcb                   RegCB-elim exploration\n  --regcbopt                RegCB optimistic exploration\n  --mellowness arg (=0.1, ) RegCB mellowness parameter c_0. Default 0.1\n  --greedify                always update first policy once in bagging\n  --cb_min_cost arg (=0, )  lower bound on cost\n  --cb_max_cost arg (=1, )  upper bound on cost\n  --first_only              Only explore the first action in a tie-breaking \n                            event\n  --lambda arg (=1, )       parameter for softmax\n  --cb_type arg             contextual bandit method to use in {ips,dr,mtr}. \n                            Default: mtr\nContextual Bandit Exploration:\n  --cb_explore arg        Online explore-exploit for a &lt;k&gt; action contextual \n                          bandit problem\n  --first arg             tau-first exploration\n  --epsilon arg (=0.05, ) epsilon-greedy exploration\n  --bag arg               bagging-based exploration\n  --cover arg             Online cover based exploration\n  --psi arg (=1, )        disagreement parameter for cover\nMultiworld Testing Options:\n  --multiworld_test arg Evaluate features as a policies\n  --learn arg           Do Contextual Bandit learning on &lt;n&gt; classes.\n  --exclude_eval        Discard mwt policy features before learning\nContextual Bandit with Action Dependent Features:\n  --cb_adf              Do Contextual Bandit learning with multiline action \n                        dependent features.\n  --rank_all            Return actions sorted by score order\n  --no_predict          Do not do a prediction when training\n  --cb_type arg         contextual bandit method to use in {ips, dm, dr, mtr, \n                        sm}. Default: mtr\nContextual Bandit Options:\n  --cb arg              Use contextual bandit learning with &lt;k&gt; costs\n  --cb_type arg         contextual bandit method to use in {ips,dm,dr}\n  --eval                Evaluate a policy rather than optimizing.\nCost Sensitive One Against All with Label Dependent Features:\n  --csoaa_ldf arg       Use one-against-all multiclass learning with label \n                        dependent features.\n  --ldf_override arg    Override singleline or multiline from csoaa_ldf or \n                        wap_ldf, eg if stored in file\n  --csoaa_rank          Return actions sorted by score order\n  --probabilities       predict probabilites of all classes\nCost Sensitive One Against All with Label Dependent Features:\n  --wap_ldf arg         Use weighted all-pairs multiclass learning with label \n                        dependent features.  Specify singleline or multiline.\nInteract via elementwise multiplication:\n  --interact arg        Put weights on feature products from namespaces &lt;n1&gt; \n                        and &lt;n2&gt;\nCost Sensitive One Against All:\n  --csoaa arg           One-against-all multiclass with &lt;k&gt; costs\nCost-sensitive Active Learning:\n  --cs_active arg                       Cost-sensitive active learning with &lt;k&gt;\n                                        costs\n  --simulation                          cost-sensitive active learning \n                                        simulation mode\n  --baseline                            cost-sensitive active learning baseline\n  --domination arg (=1, )               cost-sensitive active learning use \n                                        domination. Default 1\n  --mellowness arg (=0.1, )             mellowness parameter c_0. Default 0.1.\n  --range_c arg (=0.5, )                parameter controlling the threshold for\n                                        per-label cost uncertainty. Default \n                                        0.5.\n  --max_labels arg (=18446744073709551615, )\n                                        maximum number of label queries.\n  --min_labels arg (=18446744073709551615, )\n                                        minimum number of label queries.\n  --cost_max arg (=1, )                 cost upper bound. Default 1.\n  --cost_min arg (=0, )                 cost lower bound. Default 0.\n  --csa_debug                           print debug stuff for cs_active\nMultilabel One Against All:\n  --multilabel_oaa arg  One-against-all multilabel with &lt;k&gt; labels\nimportance weight classes:\n  --classweight arg     importance weight multiplier for class\nMemory Tree:\n  --memory_tree arg (=0, )             Make a memory tree with at most &lt;n&gt; \n                                       nodes\n  --max_number_of_labels arg (=10, )   max number of unique label\n  --leaf_example_multiplier arg (=1, ) multiplier on examples per leaf (default\n                                       = log nodes)\n  --alpha arg (=0.1, )                 Alpha\n  --dream_repeats arg (=1, )           number of dream operations per example \n                                       (default = 1)\n  --top_K arg (=1, )                   top K prediction error (default 1)\n  --learn_at_leaf                      whether or not learn at leaf (defualt = \n                                       True)\n  --oas                                use oas at the leaf\n  --dream_at_update arg (=0, )         turn on dream operations at reward based\n                                       update as well\n  --online                             turn on dream operations at reward based\n                                       update as well\nRecall Tree:\n  --recall_tree arg       Use online tree for multiclass\n  --max_candidates arg    maximum number of labels per leaf in the tree\n  --bern_hyper arg (=1, ) recall tree depth penalty\n  --max_depth arg         maximum depth of the tree, default log_2 (#classes)\n  --node_only             only use node features, not full path features\n  --randomized_routing    randomized routing\nLogarithmic Time Multiclass Tree:\n  --log_multi arg              Use online tree for multiclass\n  --no_progress                disable progressive validation\n  --swap_resistance arg (=4, ) disable progressive validation\n  --swap_resistance arg (=4, ) higher = more resistance to swap, default=4\nError Correcting Tournament Options:\n  --ect arg                Error correcting tournament with &lt;k&gt; labels\n  --error arg (=0, )       errors allowed by ECT\n  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 \n                           or poisson\nBoosting:\n  --boosting arg        Online boosting with &lt;N&gt; weak learners\n  --gamma arg (=0.1, )  weak learner&#39;s edge (=0.1), used only by online BBM\n  --alg arg (=BBM, )    specify the boosting algorithm: BBM (default), logistic\n                        (AdaBoost.OL.W), adaptive (AdaBoost.OL)\nOne Against All Options:\n  --oaa arg             One-against-all multiclass with &lt;k&gt; labels\n  --oaa_subsample arg   subsample this number of negative examples when \n                        learning\n  --probabilities       predict probabilites of all classes\n  --scores              output raw scores per class\nTop K:\n  --top arg             top k recommendation\nExperience Replay:\n  --replay_m arg              use experience replay at a specified level \n                              [b=classification/regression, m=multiclass, \n                              c=cost sensitive] with specified buffer size\n  --replay_m_count arg (=1, ) how many times (in expectation) should each \n                              example be played (default: 1 = permuting)\nBinary loss:\n  --binary              report loss as binary classification on -1,1\nBootstrap:\n  --bootstrap arg       k-way bootstrap by online importance resampling\n  --bs_type arg         prediction type {mean,vote}\nscorer options:\n  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 \n                           or poisson\nStagewise polynomial options:\n  --stage_poly                use stagewise polynomial feature learning\n  --sched_exponent arg (=1, ) exponent controlling quantity of included \n                              features\n  --batch_sz arg (=1000, )    multiplier on batch size before including more \n                              features\n  --batch_sz_no_doubling      batch_sz does not double\nLow Rank Quadratics FA:\n  --lrqfa arg           use low rank quadratic features with field aware \n                        weights\nLow Rank Quadratics:\n  --lrq arg             use low rank quadratic features\n  --lrqdropout          use dropout training for low rank quadratic features\nAutolink:\n  --autolink arg        create link function with polynomial d\nVW options:\n  --marginal arg                   substitute marginal label estimates for ids\n  --initial_denominator arg (=1, ) initial denominator\n  --initial_numerator arg (=0.5, ) initial numerator\n  --compete                        enable competition with marginal features\n  --update_before_learn            update marginal values before learning\n  --unweighted_marginals           ignore importance weights when computing \n                                   marginals\n  --decay arg (=0, )               decay multiplier per event (1e-3 for \n                                   example)\nMatrix Factorization Reduction:\n  --new_mf arg          rank for reduction-based matrix factorization\nNeural Network:\n  --nn arg              Sigmoidal feedforward network with &lt;k&gt; hidden units\n  --inpass              Train or test sigmoidal feedforward network with input \n                        passthrough.\n  --multitask           Share hidden layer across all reduced tasks.\n  --dropout             Train or test sigmoidal feedforward network using \n                        dropout.\n  --meanfield           Train or test sigmoidal feedforward network using mean \n                        field.\nConfidence:\n  --confidence                 Get confidence for binary predictions\n  --confidence_after_training  Confidence after training\nActive Learning with Cover:\n  --active_cover                enable active learning with cover\n  --mellowness arg (=8, )       active learning mellowness parameter c_0. \n                                Default 8.\n  --alpha arg (=1, )            active learning variance upper bound parameter \n                                alpha. Default 1.\n  --beta_scale arg (=3.16228, ) active learning variance upper bound parameter \n                                beta_scale. Default sqrt(10).\n  --cover arg (=12, )           cover size. Default 12.\n  --oracular                    Use Oracular-CAL style query or not. Default \n                                false.\nActive Learning:\n  --active                enable active learning\n  --simulation            active learning simulation mode\n  --mellowness arg (=8, ) active learning mellowness parameter c_0. Default 8\nExperience Replay:\n  --replay_b arg              use experience replay at a specified level \n                              [b=classification/regression, m=multiclass, \n                              c=cost sensitive] with specified buffer size\n  --replay_b_count arg (=1, ) how many times (in expectation) should each \n                              example be played (default: 1 = permuting)\nBaseline options:\n  --baseline            Learn an additive baseline (from constant features) and\n                        a residual separately in regression.\n  --lr_multiplier arg   learning rate multiplier for baseline model\n  --global_only         use separate example with only global constant for \n                        baseline predictions\n  --check_enabled       only use baseline when the example contains enabled \n                        flag\nOjaNewton options:\n  --OjaNewton                    Online Newton with Oja&#39;s Sketch\n  --sketch_size arg (=10, )      size of sketch\n  --epoch_size arg (=1, )        size of epoch\n  --alpha arg (=1, )             mutiplicative constant for indentiy\n  --alpha_inverse arg            one over alpha, similar to learning rate\n  --learning_rate_cnt arg (=2, ) constant for the learning rate 1/t\n  --normalize arg                normalize the features or not\n  --random_init arg              randomize initialization of Oja or not\nLBFGS and Conjugate Gradient options:\n  --conjugate_gradient  use conjugate gradient based optimization\nLBFGS and Conjugate Gradient options:\n  --bfgs                       use conjugate gradient based optimization\n  --hessian_on                 use second derivative in line search\n  --mem arg (=15, )            memory in bfgs\n  --termination arg (=0.001, ) Termination threshold\nLatent Dirichlet Allocation:\n  --lda arg                    Run lda with &lt;int&gt; topics\n  --lda_alpha arg (=0.1, )     Prior on sparsity of per-document topic weights\n  --lda_rho arg (=0.1, )       Prior on sparsity of topic distributions\n  --lda_D arg (=10000, )       Number of documents\n  --lda_epsilon arg (=0.001, ) Loop convergence threshold\n  --minibatch arg (=1, )       Minibatch size, for LDA\n  --math-mode arg (=0, )       Math mode: simd, accuracy, fast-approx\n  --metrics                    Compute metrics\nNoop Learner:\n  --noop                do no learning\nPrint psuedolearner:\n  --print               print examples\nGradient Descent Matrix Factorization:\n  --rank arg            rank for matrix factorization.\n  --bfgs                Option not supported by this reduction\n  --conjugate_gradient  Option not supported by this reduction\nNetwork sending:\n  --sendto arg          send examples to &lt;host&gt;\nStochastic Variance Reduced Gradient:\n  --svrg                  Streaming Stochastic Variance Reduced Gradient\n  --stage_size arg (=1, ) Number of passes per SVRG stage\nFollow the Regularized Leader:\n  --ftrl                FTRL: Follow the Proximal Regularized Leader\n  --coin                Coin betting optimizer\n  --pistol              PiSTOL: Parameter-free STOchastic Learning\n  --ftrl_alpha arg      Learning rate for FTRL optimization\n  --ftrl_beta arg       Learning rate for FTRL optimization\nKernel SVM:\n  --ksvm                   kernel svm\n  --reprocess arg (=1, )   number of reprocess steps for LASVM\n  --pool_greedy            use greedy selection on mini pools\n  --para_active            do parallel active learning\n  --pool_size arg (=1, )   size of pools for active learning\n  --subsample arg (=1, )   number of items to subsample from the pool\n  --kernel arg (=linear, ) type of kernel (rbf or linear (default))\n  --bandwidth arg (=1, )   bandwidth of rbf kernel\n  --degree arg (=2, )      degree of poly kernel\nGradient Descent options:\n  --sgd                  use regular stochastic gradient descent update.\n  --adaptive             use adaptive, individual learning rates.\n  --adax                 use adaptive learning rates with x^2 instead of g^2x^2\n  --invariant            use safe/importance aware updates.\n  --normalized           use per feature normalized updates\n  --sparse_l2 arg (=0, ) use per feature normalized updates\n  --l1_state arg (=0, )  use per feature normalized updates\n  --l2_state arg (=1, )  use per feature normalized updates\nInput options:\n  -d [ --data ] arg     Example set\n  --daemon              persistent daemon mode on port 26542\n  --foreground          in persistent daemon mode, do not run in the background\n  --port arg            port to listen on; use 0 to pick unused port\n  --num_children arg    number of children for persistent daemon mode\n  --pid_file arg        Write pid file in persistent daemon mode\n  --port_file arg       Write port used in persistent daemon mode\n  -c [ --cache ]        Use a cache.  The default is &lt;data&gt;.cache\n  --cache_file arg      The location(s) of cache_file.\n  --json                Enable JSON parsing.\n  --dsjson              Enable Decision Service JSON parsing.\n  -k [ --kill_cache ]   do not reuse existing cache: create a new one always\n  --compressed          use gzip format whenever possible. If a cache file is \n                        being created, this option creates a compressed cache \n                        file. A mixture of raw-text &amp; compressed inputs are \n                        supported with autodetection.\n  --no_stdin            do not default to reading from stdin\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["import sys\nsys.path.append('../..')\n\nimport os\nfrom subprocess import run\nfrom tempfile import TemporaryDirectory\nfrom time import process_time\n\nimport pandas as pd\nimport papermill as pm\n\nfrom reco_utils.common.notebook_utils import is_jupyter\nfrom reco_utils.dataset.movielens import load_pandas_df\nfrom reco_utils.dataset.python_splitters import python_random_split\nfrom reco_utils.evaluation.python_evaluation import (rmse, mae, exp_var, rsquared, get_top_k_items,\n                                                     map_at_k, ndcg_at_k, precision_at_k, recall_at_k)\n\nprint(\"System version: {}\".format(sys.version))\nprint(\"Pandas version: {}\".format(pd.__version__))"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">System version: 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n[GCC 7.2.0]\nPandas version: 0.23.0\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["def to_vw(df, output, logistic=False):\n    \"\"\"Convert Pandas DataFrame to vw input format\n    Args:\n        df (pd.DataFrame): input DataFrame\n        output (str): path to output file\n        logistic (bool): flag to convert label to logistic value\n    \"\"\"\n    with open(output, 'w') as f:\n        tmp = df.reset_index()\n\n        # vw の書式設定を簡略化するに評価タイプを整数にリセットする必要がある\n        tmp['rating'] = tmp['rating'].astype('int64')\n        \n        # 評価をバイナリ値に変換する\n        if logistic:\n            tmp['rating'] = tmp['rating'].apply(lambda x: 1 if x >= 3 else -1)\n        \n        # 各行を VW 入力形式に変換する (https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Input-format) [ラベル]\n        # [タグ] |[ユーザー名空間][ユーザー ID 機能] |[項目の名前空間][ムービー ID 機能] ラベルは真の評価であり、\n        # タグは予測を真実のユーザーにリンクするために使用される例の一意の ID であり、項目の名前空間は、コマンド ライン\n        # オプションを通じて相互作用機能をサポートする機能を分離します。\n        for _, row in tmp.iterrows():\n            f.write('{rating:d} {index:d}|user {userID:d} |item {itemID:d}\\n'.format_map(row))"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["def run_vw(train_params, test_params, test_data, prediction_path, logistic=False):\n    \"\"\"Convenience function to train, test, and show metrics of interest\n    Args:\n        train_params (str): vw training parameters\n        test_params (str): vw testing parameters\n        test_data (pd.dataFrame): test data\n        prediction_path (str): path to vw prediction output\n        logistic (bool): flag to convert label to logistic value\n    Returns:\n        (dict): metrics and timing information\n    \"\"\"\n\n    # モデルのトレーニング\n    train_start = process_time()\n    run(train_params.split(' '), check=True)\n    train_stop = process_time()\n    \n    # モデルのテスト\n    test_start = process_time()\n    run(test_params.split(' '), check=True)\n    test_stop = process_time()\n    \n    # 予測の読み取り\n    pred_df = pd.read_csv(prediction_path, delim_whitespace=True, names=['prediction'], index_col=1).join(test_data)\n    pred_df.drop(\"rating\", axis=1, inplace=True)\n\n    test_df = test_data.copy()\n    if logistic:\n        # メトリックが正しくキャプチャできるように完全なバイナリ ラベルを作成\n        test_df['rating'] = test['rating'].apply(lambda x: 1 if x >= 3 else -1)\n    else:\n        # 結果が正しい範囲の整数であることを確認する\n        pred_df['prediction'] = pred_df['prediction'].apply(lambda x: int(max(1, min(5, round(x)))))\n\n    # メトリクスの計算\n    result = dict()\n    result['RMSE'] = rmse(test_df, pred_df)\n    result['MAE'] = mae(test_df, pred_df)\n    result['R2'] = rsquared(test_df, pred_df)\n    result['Explained Variance'] = exp_var(test_df, pred_df)\n    result['Train Time (ms)'] = (train_stop - train_start) * 1000\n    result['Test Time (ms)'] = (test_stop - test_start) * 1000\n    \n    return result"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# データ ファイルを管理するための一時ディレクトリの作成\ntmpdir = TemporaryDirectory()\n\nmodel_path = os.path.join(tmpdir.name, 'vw.model')\nsaved_model_path = os.path.join(tmpdir.name, 'vw_saved.model')\ntrain_path = os.path.join(tmpdir.name, 'train.dat')\ntest_path = os.path.join(tmpdir.name, 'test.dat')\ntrain_logistic_path = os.path.join(tmpdir.name, 'train_logistic.dat')\ntest_logistic_path = os.path.join(tmpdir.name, 'test_logistic.dat')\nprediction_path = os.path.join(tmpdir.name, 'prediction.dat')\nall_test_path = os.path.join(tmpdir.name, 'new_test.dat')\nall_prediction_path = os.path.join(tmpdir.name, 'new_prediction.dat')"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/tmp/tmp10w3i2gd\n/tmp/tmp10w3i2gd/prediction.dat\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["# 1. データの読み取りと変形"],"metadata":{}},{"cell_type":"code","source":["# MovieLens データサイズの選択: 100k, 1m, 10m, or 20m\nMOVIELENS_DATA_SIZE = '100k'\nTOP_K = 10"],"metadata":{"tags":["parameters"],"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# MovieLens データの読み込み\ndf = load_pandas_df(MOVIELENS_DATA_SIZE)\n\n# トレーニングとテストセット用にデータを分割し、デフォルト値は各ユーザーの評価の75%をトレーニング用として、25%をテスト用として受け取る\ntrain, test = python_random_split(df, 0.75)\n\n# トレーニング、テスト データを vw フォーマットで保存する\nto_vw(df=train, output=train_path)\nto_vw(df=test, output=test_path)\n\n# ロジスティクス回帰用のデータを保存する (ラベルの調整が必要)\nto_vw(df=train, output=train_logistic_path, logistic=True)\nto_vw(df=test, output=test_logistic_path, logistic=True)"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r0.00B [00:00, ?B/s]\r  0%|          | 0.00/4.92M [00:00&lt;?, ?B/s]\r  2%|▏         | 106k/4.92M [00:00&lt;00:06, 735kB/s]\r 18%|█▊        | 885k/4.92M [00:00&lt;00:04, 992kB/s]\r 87%|████████▋ | 4.29M/4.92M [00:00&lt;00:00, 1.40MB/s]\r4.93MB [00:00, 9.13MB/s]                            \n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["# 2. 回帰ベースのレコメンデーション\n機械学習の問題を解決するためのさまざまなアプローチを検討する場合、パフォーマンス、時間、およびリソース (メモリまたは CPU) の使用状況全体において、より複雑なソリューションがどのように実行されるかを理解するためのベースライン アプローチを生成すると役立ちます。\n\n回帰ベースのアプローチは、多くの ML 問題に対して考慮すべき最も簡単で最速のベースラインの一つです。"],"metadata":{}},{"cell_type":"markdown","source":["## 2.1 線形回帰\nデータは 1 から 5 の数値での評価を提供しており、これらの値を線形回帰モデルに適合させるのは簡単なアプローチです。このモデルは、目的変数としての評価の例と、独立した機能としての対応するユーザー ID とムービー ID に関するトレーニングを受けます。\n\n各ユーザー項目の評価を例として渡すことで、モデルは各ユーザーの平均評価とアイテムごとの平均評価に基づいて重みを学習し始めます。\n\nただし、これは整数でなくなった予測評価を生成できるため、必要に応じて 1 から 5 の整数スケールに戻すには、予測時にいくつかの追加の調整を行う必要があります。ここでは、これを評価関数で行います。"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\n使用されるコマンド ライン パラメータの簡単な説明\n  その他のオプションパラメータは、こちらで確認できます: https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-Line-Arguments\n  VW はデフォルトで線形回帰を使用するので、コマンド ライン オプションはありません。\n  -f <model_path>: トレーニング後の最終的なモデル ファイルを保持する場所を示します。\n  -d <data_path>: トレーニングまたはテストに使用するデータ ファイルを示します。\n  --quiet: これは、Quiet モードで vw を実行します (デバッグの場合は、Quiet モードを使用しないことに役立ちます)\n  -i <model_path>: トレーニング中に作成した以前のモデル ファイルを読み込む場所を示します。\n  -t: これは推論のみを実行します (モデルに対して学習の更新を行いません)\n  -p <prediction_path>: 予測出力を格納する場所を示します。\n\"\"\"\ntrain_params = 'vw -f {model} -d {data} --quiet'.format(model=model_path, data=train_path)\n# 後でトップ K 分析で使用するために結果を保存する\ntest_params = 'vw -i {model} -d {data} -t -p {pred} --quiet'.format(model=model_path, data=test_path, pred=prediction_path)\n\nresult = run_vw(train_params=train_params, \n                test_params=test_params, \n                test_data=test, \n                prediction_path=prediction_path)\n\ncomparison = pd.DataFrame(result, index=['Linear Regression'])\ncomparison"],"metadata":{"scrolled":true,"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: \n                       RMSE       ...        Test Time (ms)\nLinear Regression  0.988433       ...              4.166019\n\n[1 rows x 6 columns]\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["## 2.2 相互作用フィーチャーと線形回帰\n\nこれ以前は、ユーザーのフィーチャーとアイテムの機能を個別に扱いましたが、機能間の相互作用を考慮すると、ユーザーのよりきめ細かい環境設定を学習するメカニズムを提供できます。\n\n相互作用機能を生成するには、二次コマンド ライン引数を使用し、各文字の最初の文字に基づいてユーザーと項目の名前空間を組み合わせて結合するオプション '-q ui' を組み合わせて名前空間を指定します。\n\n現在使用される userID および itemID は、フィーチャ ID が直接使用される整数であり、例えばユーザ ID 123 評価ムービー 456 の場合、トレーニング例はフィーチャ 123 および 456 の値に 1 を入れる。ただし、インタラクションが指定されている場合 (またはフィーチャが文字列の場合)、結果として得られるインタラクションフィーチャは使用可能なフィーチャ空間にハッシュされます。フィーチャ ハッシュは、非常に疎な高次元フィーチャ空間を取り込み、より低い次元空間に減らす方法です。これにより、フィーチャとモデルの重み付けの高速計算を維持しながら、メモリを削減できます。\n\n機能ハッシュの注意点は、ハッシュ衝突が発生し、別々のフィーチャが同じ場所にマップされる可能性があることです。この場合、高いカーディナリティのフィーチャ間の相互作用をサポートするためにスペースのサイズを大きくすると有益です。使用可能なフィーチャスペースは --bit_precision (-b) 引数で指定されます。モデル内のすべてのフィーチャーの使用可能な合計スペースは 2<sup>N</sup> です。\n\n詳細については、[フィーチャー ハッシュと抽出](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Feature-Hashing-and-Extraction)を参照してください。"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\n使用されるコマンド ライン パラメータの簡単な説明\n  -b <N>: メモリ サイズを 2<sup>N</sup> エントリに設定します。\n  -q <ab>: 'a' と 'b' で始まる名前空間のフィーチャ間の二次フィーチャの相互作用を作成する\n\"\"\"\ntrain_params = 'vw -b 26 -q ui -f {model} -d {data} --quiet'.format(model=saved_model_path, data=train_path)\ntest_params = 'vw -i {model} -d {data} -t -p {pred} --quiet'.format(model=saved_model_path, data=test_path, pred=prediction_path)\n\nresult = run_vw(train_params=train_params,\n                test_params=test_params,\n                test_data=test,\n                prediction_path=prediction_path)\nsaved_result = result\n\ncomparison = comparison.append(pd.DataFrame(result, index=['Linear Regression w/ Interaction']))\ncomparison"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: \n                                      RMSE       ...        Test Time (ms)\nLinear Regression                 0.988433       ...              4.166019\nLinear Regression w/ Interaction  0.985921       ...              4.636201\n\n[2 rows x 6 columns]\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["## 2.3 多項式ロジスティック回帰\n\n線形回帰の代わりに、各評価値を個別のクラスとして扱う多項ロジスティック回帰または多クラス分類を活用します。\n\nこれにより、整数以外の結果は回避されますが、異なる評価レベルのカウントが歪んでいるとパフォーマンスが低下する可能性がある各クラスのトレーニング データも減少します。\n\n基本的なマルチクラスロジスティック回帰は、N がクラスの数であり、使用する損失関数のロジスティック オプションを証明する '--oaa N' オプションで指定された 1対全 アプローチを使用して実行できます。"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\n使用されるコマンド ライン パラメータの簡単な説明\n  --損失関数ロジスティクス: ロジスティック回帰のモデル損失関数を設定します。\n  --oaa <N>: 1対全 アプローチを使用して N の別々のモデルを列車 (すべてのモデルは単一のモデル ファイルにキャプチャされます)\n             これにより、ラベルが 1 から始まる連続した整数であることが想定されます。\n  --リンク ロジスティクス:予測出力をロジットから確率に変換します。\n予測される出力は、最も可能性の高いモデル(ラベル)です。\n\"\"\"\ntrain_params = 'vw --loss_function logistic --oaa 5 -f {model} -d {data} --quiet'.format(model=model_path, data=train_path)\ntest_params = 'vw --link logistic -i {model} -d {data} -t -p {pred} --quiet'.format(model=model_path, data=test_path, pred=prediction_path)\n\nresult = run_vw(train_params=train_params,\n                test_params=test_params,\n                test_data=test,\n                prediction_path=prediction_path)\n\ncomparison = comparison.append(pd.DataFrame(result, index=['Multinomial Regression']))\ncomparison"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: \n                                      RMSE       ...        Test Time (ms)\nLinear Regression                 0.988433       ...              4.166019\nLinear Regression w/ Interaction  0.985921       ...              4.636201\nMultinomial Regression            1.112780       ...              4.941106\n\n[3 rows x 6 columns]\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["## 2.4 ロジスティック回帰\n\nさらに、ユーザーがアイテムを好きか嫌いかに興味を持つ場合、(1,3] は悪い評価(否定的な結果)で、(3,5] は良い評価(肯定的な結果)などと、入力データをバイナリの結果として表すように調整することができます。\n\nこのフレーミングを使用すると、単純なロジスティック回帰モデルを適用できます。ロジスティック回帰を実行するには、loss_function パラメータが'ロジスティック' に変更され、ターゲットラベルが [0, 1] に切り替わります。また、予測中に '--link logistic' を設定して、ロジット出力を確率値に戻してください。"],"metadata":{}},{"cell_type":"code","source":["train_params = 'vw --loss_function logistic -f {model} -d {data} --quiet'.format(model=model_path, data=train_logistic_path)\ntest_params = 'vw --link logistic -i {model} -d {data} -t -p {pred} --quiet'.format(model=model_path, data=test_logistic_path, pred=prediction_path)\n\nresult = run_vw(train_params=train_params,\n                test_params=test_params,\n                test_data=test,\n                prediction_path=prediction_path,\n                logistic=True)\n\ncomparison = comparison.append(pd.DataFrame(result, index=['Logistic Regression']))\ncomparison"],"metadata":{"scrolled":true,"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: \n                                      RMSE       ...        Test Time (ms)\nLinear Regression                 0.988433       ...              4.166019\nLinear Regression w/ Interaction  0.985921       ...              4.636201\nMultinomial Regression            1.112780       ...              4.941106\nLogistic Regression               0.717475       ...              4.388976\n\n[4 rows x 6 columns]\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["# 3. 行列因子化に基づくレコメンデーション\n\n上記のアプローチはすべて回帰モデルをトレーニングしますが、VW は 2 つの異なるアプローチで行列因子化もサポートしています。\n\n行列因子化は、回帰モデルをトレーニングする際に特定のユーザー、アイテム、および相互作用に対して直接的な重みを学習するのではなく、ユーザーがアイテムを評価する方法を決定する潜在因子を学習しようとします。これがどのように機能するかの例としては、ジャンル別にユーザー設定と項目分類を表すことができる場合があります。ジャンルのセットが小さい場合は、各項目が各ジャンルクラスにどれだけ属しているかを関連付け、各ジャンルのユーザーの好みに合った重みを設定できます。重みの両方のセットは、内部製品がユーザー項目の評価になるベクトルとして表すことができる。行列因子化アプローチでは、ユーザーの潜在フィーチャと項目の低ランク行列を学習し、これらの行列を組み合わせて元のユーザー項目行列を近似できるようにします。\n\n## 3.1. 単数形値分解を基にした行列因子化\n\n最初のアプローチでは、特異値分解(SVD)に基づいて行列因子化を実行し、ユーザ項目評価行列の低ランク近似を学習します。これは '--rank' コマンドライン引数を使用して呼び出されます。\n\n詳細については、[行列因子化の例](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Matrix-factorization-example)を参照してください。"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\n使用されるコマンド ライン パラメータの簡単な説明\n  --ランク <N>: 減少行列内の潜在因子の数を設定します。\n\"\"\"\ntrain_params = 'vw --rank 5 -q ui -f {model} -d {data} --quiet'.format(model=model_path, data=train_path)\ntest_params = 'vw -i {model} -d {data} -t -p {pred} --quiet'.format(model=model_path, data=test_path, pred=prediction_path)\n\nresult = run_vw(train_params=train_params,\n                test_params=test_params,\n                test_data=test,\n                prediction_path=prediction_path)\n\ncomparison = comparison.append(pd.DataFrame(result, index=['Matrix Factorization (Rank)']))\ncomparison"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: \n                                      RMSE       ...        Test Time (ms)\nLinear Regression                 0.988433       ...              4.166019\nLinear Regression w/ Interaction  0.985921       ...              4.636201\nMultinomial Regression            1.112780       ...              4.941106\nLogistic Regression               0.717475       ...              4.388976\nMatrix Factorization (Rank)       1.010723       ...              4.889140\n\n[5 rows x 6 columns]\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["## 3.2. ファクタリングマシンベースの行列因子化\n\n [Rendel のファクタリング マシン](https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf)に基づく別のアプローチは、'--lrq' (低ランク二次) を使用して呼び出されます。このデモの LRQ の詳細について詳しくは、こちらをご覧ください。\n\nこれにより、ユーザー項目評価行列の近似値を生成するために乗算される 2 つの下位ランク行列が学習されます。この方法で行列を圧縮すると、非常に疎な相互作用機能を持つ回帰モデルを使用する場合の制限の一部を回避する汎用的な要因を学習できます。これにより、コンバージェンスが向上し、ディスク上のモデルが小さくなる可能性があります。\n\nパフォーマンスを向上させる追加用語は --lrqdropout で、トレーニング中に列を削除します。ただし、これは最適なランク サイズを大きくする傾向があります。L2 正則化などの他のパラメーターは、オーバーフィットを回避するのに役立ちます。"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\n使用されるコマンド ライン パラメータの簡単な説明\n  --lrq <abN>: 'a' と 'b' で始まる名前空間との間の二次相互作用に対するランク N の近似値を学習します。\n  --lrqdroupout: 一般化を改善するためにトレーニング中にドロップアウトを実行します。\n\"\"\"\ntrain_params = 'vw --lrq ui7 -f {model} -d {data} --quiet'.format(model=model_path, data=train_path)\ntest_params = 'vw -i {model} -d {data} -t -p {pred} --quiet'.format(model=model_path, data=test_path, pred=prediction_path)\n\nresult = run_vw(train_params=train_params,\n                test_params=test_params,\n                test_data=test,\n                prediction_path=prediction_path)\n\ncomparison = comparison.append(pd.DataFrame(result, index=['Matrix Factorization (LRQ)']))\ncomparison"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: \n                                      RMSE       ...        Test Time (ms)\nLinear Regression                 0.988433       ...              4.166019\nLinear Regression w/ Interaction  0.985921       ...              4.636201\nMultinomial Regression            1.112780       ...              4.941106\nLogistic Regression               0.717475       ...              4.388976\nMatrix Factorization (Rank)       1.010723       ...              4.889140\nMatrix Factorization (LRQ)        1.012304       ...              4.384723\n\n[6 rows x 6 columns]\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["# 4. 結論\n\n上の表は、推奨予測に使用できる VW ライブラリのアプローチの一部を示しています。相対的なパフォーマンスは、異なるデータセットに適用され、適切に調整されると変わりますが、すべてのアプローチがトレーニングできる速度 (75,000 例) とテスト (25,000 例) に注目すると便利です。"],"metadata":{}},{"cell_type":"markdown","source":["# 5. スコアリング"],"metadata":{}},{"cell_type":"markdown","source":["上記のいずれかのアプローチでモデルをトレーニングした後、モデルを使用して、オフライン バッチ モードまたはリアルタイム スコアリング モードで潜在的なユーザー ペアをスコア付けできます。次の例は、reco_utils ディレクトリのユーティリティを活用して、オフラインスコア付け出力から Top-K のレコメンデーションを生成する方法を示しています。"],"metadata":{}},{"cell_type":"code","source":["# 最初に、各ユーザーのすべての項目 (トレーニング中に見られるものを除く) のテスト セットを構築します\n\nusers = df[['userID']].drop_duplicates()\nusers['key'] = 1\n\nitems = df[['itemID']].drop_duplicates()\nitems['key'] = 1\n\nall_pairs = pd.merge(users, items, on='key').drop(columns=['key'])\n\n# トレーニングデータと組み合わせて、トレーニングでメモされたエントリのみを保持する\nmerged = pd.merge(train[['userID', 'itemID', 'rating']], all_pairs, on=[\"userID\", \"itemID\"], how=\"outer\")\nall_user_items = merged[merged['rating'].isnull()].fillna(0).astype('int64')\n\n# vw形式で保存(これはしばらく時間がかかる場合があります)\nto_vw(df=all_user_items, output=all_test_path)"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["# 新しいデータセットで保存されたモデル (相互作用を伴う線形回帰) を実行する\ntest_start = process_time()\ntest_params = 'vw -i {model} -d {data} -t -p {pred} --quiet'.format(model=saved_model_path, data=all_test_path, pred=prediction_path)\nrun(test_params.split(' '), check=True)\ntest_stop = process_time()\ntest_time = test_stop - test_start\n\n# 予測を読み込み、以前に保存した結果からトップkを取得\npred_data = pd.read_csv(prediction_path, delim_whitespace=True, names=['prediction'], index_col=1).join(all_user_items)\ntop_k = get_top_k_items(pred_data, col_rating='prediction', k=TOP_K)[['prediction', 'userID', 'itemID']]\ntop_k.head()"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[43]: \n   prediction  userID  itemID\n0    4.565870       1     318\n1    4.533308       1      64\n2    4.530738       1     408\n3    4.525888       1     603\n4    4.501398       1     483\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["# ランキング メトリックを取得する\nargs = [test, top_k]\nkwargs = dict(col_user='userID', col_item='itemID', col_rating='rating', col_prediction='prediction',\n              relevancy_method='top_k', k=TOP_K)\n\nrank_metrics = {'MAP': map_at_k(*args, **kwargs), \n                'NDCG': ndcg_at_k(*args, **kwargs),\n                'Precision': precision_at_k(*args, **kwargs),\n                'Recall': recall_at_k(*args, **kwargs)}"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["# 最終結果\nall_results = ['{k}: {v}'.format(k=k, v=v) for k, v in saved_result.items()]\nall_results += ['{k}: {v}'.format(k=k, v=v) for k, v in rank_metrics.items()]\nprint('\\n'.join(all_results))"],"metadata":{"scrolled":true,"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">RMSE: 0.9859208893212478\nMAE: 0.71292\nR2: 0.23119931215379363\nExplained Variance: 0.2313379575958101\nTrain Time (ms): 4.523516999995536\nTest Time (ms): 4.636200999996731\nMAP: 0.012535184652143394\nNDCG: 0.0965940631559385\nPrecision: 0.0977707006369427\nRecall: 0.03761253544606115\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["# 6. クリーンアップ"],"metadata":{}},{"cell_type":"code","source":["# テストの結果を記録\nif is_jupyter():\n    pm.record('rmse', saved_result['RMSE'])\n    pm.record('mae', saved_result['MAE'])\n    pm.record('rsquared', saved_result['R2'])\n    pm.record('exp_var', saved_result['Explained Variance'])\n    pm.record(\"train_time\", saved_result['Train Time (ms)'])\n    pm.record(\"test_time\", test_time)\n    pm.record('map', rank_metrics['MAP'])\n    pm.record('ndcg', rank_metrics['NDCG'])\n    pm.record('precision', rank_metrics['Precision'])\n    pm.record('recall', rank_metrics['Recall'])"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"code","source":["tmpdir.cleanup()"],"metadata":{"trusted":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["## 参考文献\n\n1. John Langford, et. al. Vowpal Wabbit Wiki. URL: https://github.com/VowpalWabbit/vowpal_wabbit/wiki\n2. Steffen Rendel. Factorization Machines. 2010 IEEE International Conference on Data Mining.\n3. Jake Hoffman. Matrix Factorization Example. URL: https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Matrix-factorization-example\n4. Paul Minero. Low Rank Quadratic Example. URL: https://github.com/VowpalWabbit/vowpal_wabbit/tree/master/demo/movielens"],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.5.4","nbconvert_exporter":"python","file_extension":".py"},"name":"vowpal_wabbit_deep_dive","notebookId":1172560913725345,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"celltoolbar":"Tags"},"nbformat":4,"nbformat_minor":0}
