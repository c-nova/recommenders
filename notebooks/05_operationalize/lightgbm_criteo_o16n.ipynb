{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## リアルタイム コンテンツ ベース の パーソナライゼーション モデルをデプロイする\n\nこのノートブックでは、企業が機械学習を使用したレコメンデーション システムを使用し、顧客に対してコンテンツ ベースのパーソナライゼーションを自動化する方法を示します。Azure Databricks は、ユーザーがアイテムに関与する確率を予測するモデルをトレーニングするために使用されます。さらにこの推論を使用して、ユーザーが最も消費する可能性が高いコンテンツに基づいてアイテムをランク付けできます。<br><br>\nこのノートブックは、[MMLSpark-LightGBM-Criteo notebook](../02_model/mmlspark_lightgbm_criteo.ipynb)でトレーニングされたコンテンツベースのパーソナライゼーションモデルなど、Spark ベースのモデル用のスケーラブルなリアルタイム スコアリング サービスを作成します。\n\n<br><br>\n### アーキテクチャ\n<img src=\"https://github.com/c-nova/recommenders/tree/master/notebooks/05_operationalize/lightgbm_criteo_arch_jp.svg\" alt=\"アーキテクチャ\">\n\n### 構成要素\nこのアーキテクチャでは、以下の構成要素を使用します:<br>\n- [Azure Blob Storage](https://azure.microsoft.com/ja-jp/services/storage/blobs/) は非構造データの大量の蓄積に最適化されたストレージです。このケースでは、入力データの格納に使用されます。<br>\n- [Azure Databricks](https://azure.microsoft.com/ja-jp/services/databricks/) はマネージドな Apache Spark クラスタであり、このケースではモデルのトレーニング及び評価に使用されます。<br>\n- [Azure Machine Learning service](https://azure.microsoft.com/ja-jp/services/machine-learning-service/) はこのシナリオでは機械学習モデルの登録に使用されます。<br>\n- [Azure Container Registry](https://azure.microsoft.com/ja-jp/services/container-registry/) は本番環境でモデルの提供に使用するために、コンテナとしてスコアリング スクリプトをパッケージするために使用されます。<br>\n- [Azure Kubernetes Service](https://azure.microsoft.com/ja-jp/services/kubernetes-service/) はトレーニングされたモデルを Web または App Services として展開するために使用されます。<br>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 前提条件\nこのノートブックを実行するには、以下の項目を前提としています:\n\n1. モデルは[mmlspark_lightgbm_criteo](https://aka.ms/recommenders/lgbm-criteo-training)ノートブックに示すように事前にトレーニングされています。\n2. このノートブックは、前提 1 でノートブックを実行するために使用されたものと同様に、 Azure Databricks ワークスペースで実行することを想定しています。\n3. (MML Spark と reco_utils が両方ともインストールされている)運用化のために準備された Databricks クラスタを使用します。 \n- 詳細については、[セットアップ](https://github.com/c-nova/Recommenders/blob/master/SETUP.md) の手順を参照してください。\n4. Azure Machine Learning Service ワークスペースは、モデルトのレーニングに使用される Azure Databricks ワークスペースと同じリージョンにセットアップされる必要があります。\n- 詳細については、[ワークスペースの作成](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/setup-create-workspace)を参照してください。\n5. Azure ML ワークスペース config.json は、Databricks では `dbfs:/aml_config/config.json` にアップロードされます。\n- [環境の構成](https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-configure-environment)および[Databricks CLI](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#access-dbfs-with-the-databricks-cli)を参照してください。\n6. Azure Container Instance (ACI) は使用中の Azure サブスクリプションで登録されます\n- 詳細については、[サポートされるサービス](https://docs.microsoft.com/ja-jp/azure/azure-resource-manager/resource-manager-supported-services#portal)を参照してください。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## スコア サービスのステップ\nこの例では、\"スコアリング サービス\" は、docker コンテナーによって実行されるファンクションです。JSON 書式のペイロードを使用して POST 要求を受け取り、事前に推定されたモデルに基づいてスコアを生成します。この例では、数値とカテゴリの一連のフィーチャーに基づいてユーザー アイテムの相互作用の確率を予測する、事前に推定したモデルを使用します。そのモデルは PySpark を使用してトレーニングされているため、[MML Spark 提供](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md) を使用してモデルを実行する単一インスタンス(Dcoker コンテナ内)にSparkセッションを作成します。受信した入力データに対して、相互作用の確率を返します。Azure Machine Learning を使用して、Docker コンテナーを作成して実行します。\n\nスコアリング サービスを作成するには、次の手順を実行します。\n\n1. Azure Machine KLearning ワークスペースのセットアップと認証\n2. 以前にトレーニングされたモデルをシリアル化し、Azure モデル 登録に追加する\n3. モデルを実行する 'スコアリング サービス' スクリプトを定義する\n4. スクリプトに必要なすべての前提条件を定義する\n5. モデル、ドライバー スクリプト、および前提条件を使用して Azure コンテナ イメージを作成する\n6. スケーラブルなプラットフォームである Azure Kubernetes サービスにコンテナ イメージをデプロイする \n7. サービスのテスト"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ライブラリと変数のセットアップ\n\n以下のいくつかのセルは、環境と変数を初期化します: ここで関連するライブラリをインポートし、変数を設定します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport json\nimport shutil\n\nfrom reco_utils.dataset.criteo import get_spark_schema, load_spark_df\nfrom reco_utils.azureml.aks_utils import qps_to_replicas, replicas_to_qps, nodes_to_replicas\n\nfrom azureml.core import Workspace\nfrom azureml.core import VERSION as azureml_version\n\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies \nfrom azureml.core.webservice import Webservice, AksWebservice\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\nfrom math import floor\n\n# Check core SDK version number\nprint(\"Azure ML SDK version: {}\".format(azureml_version))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Azure ML SDK version: 1.0.45\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## スコアリング サービス 変数の構成"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "MODEL_NAME = 'lightgbm_criteo.mml'  # この名前は、推論用ノートブックにパイプライン モデルを保存するために使用される名前と完全に一致する必要があります\nMODEL_DESCRIPTION = 'LightGBM Criteo Model'\n\n# AzureML アセットのセットアップ (名前はスペースを使用しない小文字の英数字で、3 ～ 32 文字の間でなければなりません)\n# Azure ML Web サービス\nSERVICE_NAME = 'lightgbm-criteo'\n# Azure ML コンテナ イメージ\nCONTAINER_NAME = SERVICE_NAME\nCONTAINER_RUN_TIME = 'spark-PY'\n# Azure Kubernetes Service (AKS)\nAKS_NAME = 'predict-aks'\n\n# 使用するその他のファイルの名前\nCONDA_FILE = \"deploy_conda.yaml\"\nDRIVER_FILE = \"mmlspark_serving.py\"",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## AzureML ワークスペースのセットアップ\nワークスペース構成は、ポータルから取得し、Databricks にアップロードできます<br>\n[AzureML on Databricks](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#azure-databricks)を参照してください。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config('/dbfs/aml_config/config.json')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## シリアル化されたモデルを準備する\n\ndocker コンテナを作成するには、最初に作成する docker コンテナーがアクセスできるように、前の手順で推定したモデルを準備します。これを行うには、モデルをワークスペースに *登録します* (詳細については、Azure ML [ドキュメント](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-model-management-and-deployment)を参照してください)。\n\nモデルは dbfs のディレクトリとして保存されており、登録する前にプロセスを容易にするためにいくつかの追加手順を実行します。\n\n### 入力スキーマ\n\nSpark サービングには、生の入力データのスキーマが必要です。したがってスキーマを取得し、モデル ディレクトリに追加のファイルとして格納します。\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "raw_schema = get_spark_schema()\nwith open(os.path.join('/dbfs', MODEL_NAME, 'schema.json'), 'w') as f:\n  f.write(raw_schema.json())",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### dbfs からローカルにモデルをコピーする\n\nローカル ファイル API を使用して DBFS 上のファイルにアクセスできますが、ローカル ファイル API は 2 GB 未満のファイルにのみアクセスできるため、dbfs との間で保存されたモデルを明示的にコピーする方が安全です (詳細は[こちら](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#access-dbfs-using-local-file-apis)を参照してください。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "model_local = os.path.join(os.getcwd(), MODEL_NAME)\ndbutils.fs.cp('dbfs:/' + MODEL_NAME, 'file:' + model_local, recurse=True)",
      "execution_count": 13,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>True\n</div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### モデルの登録\n\nこれで、Azure Machine Learning ワークスペースにモデルを登録する準備ができました。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# まずデータ転送を最小限に抑えるためにモデル ディレクトリを圧縮します\nzip_file = shutil.make_archive(base_name=MODEL_NAME, format='zip', root_dir=model_local)\n\n# モデルの登録\nmodel = Model.register(model_path=zip_file,  # ここではローカル ファイルをポイントします\n                       model_name=MODEL_NAME,  # これはモデルの登録名です\n                       description=MODEL_DESCRIPTION,\n                       workspace=ws)\n\nprint(model.name, model.description, model.version)",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model lightgbm_criteo.mml\nlightgbm_criteo.mml LightGBM Criteo Model 4\n</div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## スコアリング スクリプトの定義\n\n次に、サービスが呼び出されたときに実行されるドライバー スクリプトを作成する必要があります。スコアリングのために定義する必要がある関数は `init()` と `run()` です。`init()` 関数は、サービスの作成時に実行され、サービスが呼び出されるたびに `run()` 関数が実行されます。\n\nこの例では、`init()` 関数を使用してすべてのライブラリを読み込み、Spark セッションを初期化し、Spark ストリーミング サービスを開始し、モデル パイプラインを読み込みます。`run()` メソッドを使用して入力を Spark ストリーミング サービスにルーティングし、予測 (この場合はインタラクションの確率) を生成し、出力を返します。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "driver_file = '''\nimport os\nimport json\nfrom time import sleep\nfrom uuid import uuid4\nfrom zipfile import ZipFile\n\nfrom azureml.core.model import Model\nfrom pyspark.ml import PipelineModel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\nimport requests\n\n\ndef init():\n    \"\"\"One time initialization of pyspark and model server\"\"\"\n\n    spark = SparkSession.builder.appName(\"Model Server\").getOrCreate()\n    import mmlspark  # this is needed to load mmlspark libraries\n\n    # extract and load model\n    model_path = Model.get_model_path('{model_name}')\n    with ZipFile(model_path, 'r') as f:\n        f.extractall('model')\n    model = PipelineModel.load('model')\n\n    # load data schema saved with model\n    with open(os.path.join('model', 'schema.json'), 'r') as f:\n        schema = StructType.fromJson(json.load(f))\n\n    input_df = (\n        spark.readStream.continuousServer()\n        .address(\"localhost\", 8089, \"predict\")\n        .load()\n        .parseRequest(schema)\n    )\n\n    output_df = (\n        model.transform(input_df)\n        .makeReply(\"probability\")\n    )\n\n    checkpoint = os.path.join('/tmp', 'checkpoints', uuid4().hex)\n    server = (\n        output_df.writeStream.continuousServer()\n        .trigger(continuous=\"30 seconds\")\n        .replyTo(\"predict\")\n        .queryName(\"prediction\")\n        .option(\"checkpointLocation\", checkpoint)\n        .start()\n    )\n\n    # let the server finish starting\n    sleep(1)\n\n\ndef run(input_json):\n    try:\n        response = requests.post(data=input_json, url='http://localhost:8089/predict')\n        result = response.json()['probability']['values'][1]\n    except Exception as e:\n        result = str(e)\n    \n    return json.dumps({{\"result\": result}})\n    \n'''.format(model_name=MODEL_NAME)\n\n# check syntax\nexec(driver_file)\n\nwith open(DRIVER_FILE, \"w\") as f:\n    f.write(driver_file)",
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 依存関係の定義\n\n次に、ドライバー スクリプトで必要な依存関係を定義します。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# azureml-sdk は登録されたモデルを読み込むために必要です\nconda_file = CondaDependencies.create(pip_packages=['azureml-sdk', 'requests']).serialize_to_string()\n\nwith open(CONDA_FILE, \"w\") as f:\n    f.write(conda_file)",
      "execution_count": 19,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## イメージを作成する\n\n`ContainerImage` クラスを使用して、最初に定義されたドライバーと依存関係を使用してイメージを構成し、次に後で使用するイメージを作成します。<br>\nイメージをビルドすると、Docker を使用してローカルにダウンロードおよびデバッグすることができます。[トラブルシューティング方法](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-troubleshoot-deployment)を参照してください。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "image_config = ContainerImage.image_configuration(execution_script=DRIVER_FILE, \n                                                  runtime=CONTAINER_RUN_TIME,\n                                                  conda_file=CONDA_FILE,\n                                                  tags={\"runtime\":CONTAINER_RUN_TIME, \"model\": MODEL_NAME})\n\nimage = ContainerImage.create(name=CONTAINER_NAME,\n                              models=[model],\n                              image_config=image_config,\n                              workspace=ws)\n\nimage.wait_for_creation(show_output=True)",
      "execution_count": 21,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Creating image\nRunning......................\nSucceededImage creation operation finished for image lightgbm-criteo:3, operation &quot;Succeeded&quot;\n</div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## サービスの作成\n\nイメージを作成したら、Azure Kubernetes サービス (AKS) を構成し、イメージを AKS Web Service としてデプロイします。\n\n**注** 私たちは `Webservice.deploy_from_model()` ファンクションを使用して、登録されたモデルとimage_configuration から直接サービスを作成することが*可能*です。ここではイメージを明示的に作成し、次の 3 つの理由から `deploy_from_image()` を使用します。\n\n1. 実際に行われているステップの面でより透明性を提供します。\n2. これにより多くの柔軟性および制御を提供します。たとえば、作成するサービスとは無関係な名前のイメージを作成できます。これは、イメージを複数のサービスで使用する場合に役立ちます。\n3. これにより、潜在的により速い反復およびより多くの移植性を得られます。イメージが作成されると、まったく同じコードで新しいデプロイを作成することができます。\n\n### セットアップと計画\n\n本番サービスを設定する際には、まずサポートする負荷量を見積もる必要があります。それを見積るためには、1 回の呼び出しにかかる時間を見積もる必要があります。この例では、いくつかのローカル テストを行い、1 つのクエリの処理に約 100 ミリ秒掛かると見積もっています。\n\nいくつかの追加の仮定に基づいて、1 秒あたりの目標数のクエリ(qps) をサポートするために必要なレプリカの数を見積もることができます。\n\n**注:** この見積もりは開始点としての概算数として使用する必要があり、我々はより良い見積りにブラッシュアップするために、その後のロードテストでパフォーマンスを検証することができます。詳細については、こちらの[ドキュメント](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#aks)を参照してください。\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "この種の計算をサポートするヘルパー関数をいくつか記述し、1 つのクエリを完了する時間の見積もりとして 100 ミリ秒を使用して、1 秒あたり 25、50、100、200、および 350 クエリの読み込みをサポートするために必要なレプリカの数を推定します。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_target_qps = [25, 50, 100, 200, 350]\nquery_processing_time = 0.1  ## 処理/秒\nreplica_estimates = {t: qps_to_replicas(t, query_processing_time) for t in all_target_qps}",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "顧客ベースの規模やその他の考慮事項(トラフィックを増やす可能性のある今後の予定など)に基づいて、サポートする最大負荷を決定します。この例では、1 秒あたり 100 のクエリをサポートする必要があり、対応するレプリカの数 (上記の見積もりに基づいて 15 個) を使用する必要があることを示します。\n\nレプリカの数がわかったら、Azure Kubernetes サービス内に十分なリソース (コアとメモリ) があり、その数のレプリカをサポートする必要があります。その数を見積もるためには、各レプリカに割り当てられるコアの数を知る必要があります。コアごとに複数のレプリカがあるユース ケースが多いため、この数は小数点数になる可能性があります。詳細は[こちら](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#cpu-units)をご覧ください。以下の Web サービスを作成するときは、各レプリカに 0.3 の 'cpu_cores' と 0.5 GB のメモリを割り当てます。15 のレプリカをサポートするには、`15*0.3` コアと `15*0.5` GB のメモリが必要です。\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "cpu_cores_per_replica = 0.3\nprint('{} cores required'.format(replica_estimates[100]*cpu_cores_per_replica))\nprint('{} GB of memory required'.format(replica_estimates[100]*0.5))",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "4.5 cores required\n7.5 GB of memory required\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Azure Kubernetes サービスをプロビジョニングする\n\n必要なコア数とメモリ量の見積もりができたので、AKS クラスターを構成して作成します。デフォルトでは、`AksCompute.provisioning_configuration()` は `vm_size='Standard_D3_v2'` を持つ 3 つのエージェントを持つ構成を作成します。各 Standard_D3_v2 仮想マシンには 4 つのコアと 14 GB のメモリがあるので、デフォルトでは 12 コアと 42 GB のメモリを組み合わせたクラスタが作成され、推定負荷要件を満たすのに十分です。\n\n**注**: この特定のケースでは、負荷要件が 4.5 コアに過ぎない場合でも、AKS クラスターの 12 コアを下回 **らない** 必要があります。12 コアは、Web サービスに必要な AKS のコアの最小数です。[詳細](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#aks)については、ドキュメントを参照してください。`agent_count` パラメーターと `vm_size` パラメーターを使用して、負荷要件で必要な場合はコア数を 12 を超える値を増やすことができますが、それらを使用して減らすことはしないでください。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# 最初に AKS コンピュートを作成\n\n# 既定の構成を使用する (カスタマイズするパラメーターを提供することもできます)\nprov_config = AksCompute.provisioning_configuration()\n\n# クラスタの作成\naks_target = ComputeTarget.create(\n  workspace=ws, \n  name=AKS_NAME, \n  provisioning_configuration=prov_config\n)\n\naks_target.wait_for_completion(show_output=True)\n\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 検討事項\n\n推定される負荷要件は Azure Machine Learning によって設定された最小値より小さいため、Web サービスで使用するレプリカの数を見積もる別の方法を検討する必要があります。これが AKS クラスターで実行される唯一のサービスである場合、すべてのコンピューティング リソースを活用しないことでリソースを浪費している可能性があります。最初は、予想される負荷を使用して、使用するレプリカの数を見積もります。このアプローチの代わりに、クラスター内のコア数を使用して、サポートできるレプリカの最大数を推定することもできます。\n\nレプリカの最大数を見積もるためには、ベースとなる kubernetes の操作とノードのオペレーティング システムとコア機能の各ノードにオーバーヘッドがあることを考慮する必要があります。この場合は 10% のオーバーヘッドを想定していますが、詳細については[こちら](https://docs.microsoft.com/en-us/azure/aks/concepts-clusters-workloads)を参照してください。\n\n**注** この例ではコアを使用していますが、代わりにメモリ要件を活用することもできます。\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "max_replicas_12_cores = nodes_to_replicas(\n    n_cores_per_node=4, n_nodes=3, cpu_cores_per_replica=cpu_cores_per_replica\n)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "クラスターがサポートするレプリカの数が判明すると、AKS クラスタがサポートできると考えられる 1 秒あたりのクエリを推定できます。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "replicas_to_qps(max_replicas_12_cores, query_processing_time)",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "140"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Web サービスを作成する\n\n次に、Web サービスを構成して作成します。この構成では、各レプリカが `cpu_cores=cpu_cores_per_replica` (デフォルトは `cpu_cores=0.1`) を設定するとします。この値はこのサービスの経験と事前のテストに基づいて調整しています。\n\n`AksWebservice.deploy_configuration()` に引数が渡されない場合は、`autoscale_enabled=True` と共に `autoscale_min_replicas=1` と `autoscale_max_replicas=10` が設定されます。最大値は、1 秒あたり 100 クエリをサポートするための最小要件を満たしていないため、調整する必要があります。この値は、負荷 (15) に基づいて見積もりを調整するか、AKS クラスター (36) でサポートできる数に基づいて見積もりを調整できます。この例では、AKS クラスターを他のタスクまたはサービスに使用できるように、負荷に基づく値に設定します。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "webservice_config = AksWebservice.deploy_configuration(cpu_cores=cpu_cores_per_replica,\n                                                       autoscale_enabled=True,\n                                                       autoscale_max_replicas=replica_estimates[100])\n\n# 作成したイメージを使用してサービスを展開する\naks_service = Webservice.deploy_from_image(\n  workspace=ws, \n  name=SERVICE_NAME,\n  deployment_config=webservice_config,\n  image=image,\n  deployment_target=aks_target\n)\n\naks_service.wait_for_deployment(show_output=True)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## サービスをテストする\n\n次に、`サンプル` データのデータを使用してサービスをテストできます。\n\nこのサービスは JSON をペイロードとして想定しているので、サンプル データを取得し、ディクショナリに変換してからサービス エンドポイントに送信します。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# URI を表示\nurl = aks_service.scoring_uri\nprint('AKS URI: {}'.format(url))\n\n# aks_service のキーのいずれかを使用した認証のセットアップ\nheaders = dict(Authorization='Bearer {}'.format(aks_service.get_keys()[0]))",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#いくつかのサンプルデータを使用\ndf = load_spark_df(size='sample', spark=spark, dbutils=dbutils)\ndata = df.head().asDict()\nprint(data)",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# AKS クラスターに要求を送信\nresponse = requests.post(url=url, json=data, headers=headers)\nprint(response.json())",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### サービスを削除する\n\n全てが完了後、コストを最小限に抑えるためにサービスを削除できます。上記の同じコマンドを使用して、イメージからいつでも再デプロイできます。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Web サービスを削除するには、次の行のコメントを解除します\n# aks_service.delete()",
      "execution_count": 30,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "aks_service.state",
      "execution_count": 31,
      "outputs": [
        {
          "data": {
            "text/html": "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">34</span><span class=\"ansired\">]: </span>&apos;Deleting&apos;\n</div>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "pasha"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "deploy-to-aci-04",
    "notebookId": 904892461294324
  },
  "nbformat": 4,
  "nbformat_minor": 1
}