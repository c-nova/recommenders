{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n\n<i>Licensed under the MIT License.</i>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# リアルタイム レコメンデーション API 環境を構築する\n## (Azure Storage & Non-Interactive login enabled)\n\nこのリファレンス アーキテクチャは、レコメンデーションシステム構築の完全なライフ サイクルを示しています。このシナリオでは、Azure Databricks を使用して、API として展開するレコメンデーション モデルのトレーニング、適切な Azure リソースの作成を説明します。Azure Cosmos DB、Azure Machine Learning と Azure Kubernetes Service を使用します。\n\nこのアーキテクチャは、製品、映画・番組、およびニュースのレコメンデーションを含む多くの一般的なレコメンデーション エンジン シナリオに適用することができます。\n\n### アーキテクチャ\n![architecture](https://recodatasets.blob.core.windows.net/images/reco-arch.png \"Architecture\")\n\n**シナリオ**: とあるメディア企業は、自社のユーザーに映画やビデオのレコメンデーションを提供したいと考えています。パーソナライズされたレコメンデーションを提供することによって、クリックスルー率やサイトの利用率、ユーザー満足度の高い増加を通じて、いくつかのビジネス目標を満たそうとしています。\n\nこのリファレンスでは、訓練し、特定のユーザーに対してトップ 10 のレコメンドする映画を提供することが可能なリアルタイムのレコメンデーション サービス API をトレーニングし、展開します。\n\n### コンポーネント\nこのアーキテクチャには、以下の主要なコンポーネントが含まれます:\n* [Azure Storage](https://docs.microsoft.com/ja-jp/azure/storage/) は、トレーニングやテストを行うために入力として利用するデータを格納し、API 介してその他のサービスに提供します。Azure Storage は大量のデータを格納することが可能なため、ここでは元のシナリオでは毎回ダウンロードしていた MovieLens のデータを、最新版も含めて全て格納します。。\n* [Azure Databricks](https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks) は、入力データの準備と、Spark クラスター上でレコメンデーション モデルのトレーニング開発環境として使用されます。Azure Databricks はまた、データの処理や機械学習のタスクを実行するためのノートブックで共同作業を行うためのインタラクティブなワークスペースを提供します。\n* [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes)(AKS) は、Kubernetes クラスター上に機械学習モデルサービス API の展開と運用を行うために使用されます。AKS は、コンテナー化されたモデルをホストし、スループット要件を満たすためのスケーラビリティ、ID およびアクセスの管理、ログおよび状態監視を提供します。\n* [Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction) は、ユーザーごとにレコメンドされた映画のトップ 10 を格納するために使用される、グローバル分散型データベース サービスです。Azure Cosmos DB は指定されたユーザーのトップ 10 のレコメンド アイテムの読み取りに対して低レイテンシ (99 パーセンタイルにおいて 10 ms) で提供可能なため、このシナリオに最適です。\n* [Azure Machine Learning Service](https://docs.microsoft.com/en-us/azure/machine-learning/service/) は、機械学習モデルのトラッキングと管理、スケーラブルな Azure Kubernetes Service環境にこれらのモデルをパッケージ化、展開するために使用するサービスです。\n\n\n### 目次\n0. [ファイルのインポート](0-ファイルのインポート)\n1. [サービスの作成](#1-Service-Creation)\n2. [トレーニング](#2-Training)\n3. [運用化](#3.-Operationalize-the-Recommender-Service)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## セットアップ\n\nこのノートブックは Azure Databricks 上でのみ動作します。Azure Databricks のワークスペースにこのノートブックをインポートする手順については、[この](https://docs.azuredatabricks.net/user-guide/notebooks/notebook-manage.html#import-a-notebook)手順に従ってください。\n\nAzure Databricks のセットアップは、リポジトリ内の [セットアップ ガイド]($./SETUP) で、該当するセクションに従って完了する必要があります。\n\n注意事項: このノートは運用化をサポートするための依存関係を追加する**必要**があります。詳細は[セットアップ ガイド]($./SETUP)を参照してください。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 0 ファイルのインポート"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport os\nimport pandas as pd\nimport pprint\nimport shutil\nimport time, timeit\nimport urllib\nimport yaml\nimport json\nimport uuid\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom azure.common.client_factory import get_client_from_cli_profile\nfrom azure.mgmt.compute import ComputeManagementClient\nimport azure.mgmt.cosmosdb\nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.model import Model\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.compute import AksCompute, ComputeTarget\nfrom azureml.core.webservice import Webservice, AksWebservice\n\n\nimport pydocumentdb\nimport pydocumentdb.document_client as document_client\n\nimport pyspark\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n\nfrom reco_utils.dataset import movielens\nfrom reco_utils.dataset.cosmos_cli import find_collection, read_collection, read_database, find_database\nfrom reco_utils.dataset.spark_splitters import spark_random_split\nfrom reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n\nprint(\"PySpark version:\", pyspark.__version__)\nprint(\"Azure SDK version:\", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 0.1 Azure Storage の定義"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Azure Storage のアカウント名\nstorage_account_name = \"<Azure storage account name>\"\n# Azure Storage のアクセス キー（コンテナやBLOBのSASでは無いので注意）\nstorage_account_access_key = \"<Azure storage access key>\"\n\nspark.conf.set(\n  \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n  storage_account_access_key)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1 サービスの作成\n**Subscription ID**の項目を展開したいサブスクリプションIDによって変更してください。\n\n#### このノートブックで作成されるサービス:\n1. [Azure ML Service](https://docs.databricks.com/user-guide/libraries.html)\n1. [Azure Cosmos DB](https://azure.microsoft.com/ja-jp/services/cosmos-db/)\n1. [Azure Container Registery](https://docs.microsoft.com/ja-jp/azure/container-registry/)\n1. [Azure Container Instances](https://docs.microsoft.com/ja-jp/azure/container-instances/)\n1. [Azure Application Insights (Azure Monitor)](https://azure.microsoft.com/ja-jp/services/monitor/)\n1. [Azure Storage](https://docs.microsoft.com/ja-jp/azure/storage/common/storage-account-overview)\n1. [Azure Key Vault](https://azure.microsoft.com/ja-jp/services/key-vault/)\n1. [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/ja-jp/services/kubernetes-service/)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# サービス名称の設定\n# (訳注) オリジナルは uuid.uuid4() で毎回ランダムな UUID を生成しているため、再実行のたびに新たなリソースグループが作成されるため廃止\n#short_uuid = str(uuid.uuid4())[:4]\n# (訳注) 変更後は4桁の英数字を任意で選択する方式に変更\nshort_uuid = \"911c\"\nprefix = \"reco\" + short_uuid\ndata = \"mvl\"\nalgo = \"als\"\n\n# Cosmos DB のシークレット ファイルの保管場所\nsecrets_path = '/dbfs/FileStore/dbsecrets.json'\nws_config_path = '/dbfs/FileStore'\n\n# 利用するサブスクリプション ID に変更\nsubscription_id = \"<Azure Subscription ID>\"\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# リソースグループとワークスペース名の設定\nresource_group = prefix + \"_\" + data\nworkspace_name = prefix + \"_\"+data+\"_aml\"\n# リージョンの設定\nworkspace_region = \"<Azure ML region name>\"\nprint(\"Resource group:\", resource_group)\n\n# カラムの設定\nuserCol = \"UserId\"\nitemCol = \"MovieId\"\nratingCol = \"Rating\"\n\n# Cosmos DB の構成値\nlocation = workspace_region\naccount_name = resource_group + \"-ds-sql\"\n# Cosmos DB の account_name は \"_\" が利用できず、31文字未満に制限される\naccount_name = account_name.replace(\"_\",\"-\")[0:min(31,len(prefix))]\nDOCUMENTDB_DATABASE = \"recommendations\"\nDOCUMENTDB_COLLECTION = \"user_recommendations_\" + algo\n\n# Azure ML の構成値\nhistory_name = 'spark-ml-notebook'\n# 注: アセット名はスペースを含まない小文字英数のみで、30文字未満に制限\nmodel_name = data+\"-\"+algo+\"-reco.mml\"\nservice_name = data + \"-\" + algo\nexperiment_name = data + \"_\"+ algo +\"_Experiment\"\n# AKS の名称は16文字以下の \"-\" を含む英数のみが利用可能\naks_name = prefix.replace(\"_\",\"-\")[0:min(12,len(prefix))] + '-aks'\n# コンテナの名前を追加\ncontainer_image_name = '-'.join([data, algo])\n\ntrain_data_path = data + \"Train\"\ntest_data_path = data + \"Test\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.1 Azure ML ワークスペースのインポートまたは作成:\nこのコマンドは指定された Azure ML ワークスペースが存在するか否かを確認し、存在しない場合には作成を行います。\nまたこのノートブックでは Azure ML ワークスペースへのアタッチをサービス プリンシパルで行います。サービス プリンシパルの AAD への登録方法は[こちら](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azure-ml.ipynb)をご参照ください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.authentication import ServicePrincipalAuthentication\n\n# サービス プリンシパルのパスワードは本番では平文で格納しないように注意（Databricks の変数を利用する）\nsvc_pr = ServicePrincipalAuthentication(\n    tenant_id = \"<Service Principal Tenant ID>\",\n    service_principal_id = \"<Service Principal ID>\",\n    service_principal_password = \"<Service Principal Password>\"\n)\n\ntry:\n  # 既存の AML ワークスペースを利用\n  ws = Workspace(\n    subscription_id = subscription_id,\n    resource_group = resource_group,\n    workspace_name = workspace_name,\n    auth=svc_pr\n    )\n  print(\"Found workspace {} at location {}\".format(ws.name, ws.location))\nexcept :\n  # 既存の AML ワークスペースが見つからない場合には作成\n  ws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      exist_ok=True)\n  # サブスクリプションID、リソースグループ名、ワークスペース名を永続化させるために aml_config/config.json に記録\n  ws.write_config(ws_config_path)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.2 レコメンデーション結果の保存先として Cosmos DB リソースを作成:\nこのノートブックでは、Azure Client をサービス プリンシパルで作成しています。詳しい情報は[こちら](https://docs.microsoft.com/ja-jp/azure/python/python-sdk-azure-authenticate?view=azure-python)をご参照ください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.common.client_factory import get_client_from_json_dict\nfrom azure.mgmt.compute import ComputeManagementClient\n\n# az ad sp create-for-rbac --name \"MY-PRINCIPAL-NAME\" --password \"STRONG-SECRET-PASSWORD\" で生成された内容を転記\n# サービス プリンシパルのパスワードは本番では平文で格納しないように注意（Databricks の変数を利用する）\nconfig_dict = {\n  \"clientId\": \"<Generated by az cli>\",\n  \"clientSecret\": \"<Generated by az cli>\",\n  \"subscriptionId\": \"<Generated by az cli>\",\n  \"tenantId\": \"<Generated by az cli>\",\n  \"activeDirectoryEndpointUrl\": \"<Generated by az cli>\",\n  \"resourceManagerEndpointUrl\": \"<Generated by az cli>\",\n  \"activeDirectoryGraphResourceId\": \"<Generated by az cli>\",\n  \"sqlManagementEndpointUrl\": \"<Generated by az cli>\",\n  \"galleryEndpointUrl\": \"<Generated by az cli>\",\n  \"managementEndpointUrl\": \"<Generated by az cli>\"\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## (訳注) ここで認証エラーが表示される場合には、scripts/prepare_databricks_for_o16n.sh に記載されているライブラリのバージョン指定でライブラリのインストールを行ってください\n## 複数のサブスクリプションを持っているユーザーの場合に、subscription_id を明示的に渡す\nclient = get_client_from_json_dict(azure.mgmt.cosmosdb.CosmosDB, config_dict)\n\nasync_cosmosdb_create = client.database_accounts.create_or_update(\n    resource_group,\n    account_name,\n    {\n        'location': location,\n        'locations': [{\n            'location_name': location\n        }]\n    }\n)\naccount = async_cosmosdb_create.result()\n\nmy_keys = client.database_accounts.list_keys(\n    resource_group,\n    account_name\n)\n\nmaster_key = my_keys.primary_master_key\nendpoint = \"https://\" + account_name + \".documents.azure.com:443/\"\n\n# Cosmos DB クライアント\nclient = document_client.DocumentClient(endpoint, {'masterKey': master_key})\n\nif find_database(client, DOCUMENTDB_DATABASE) == False:\n    db = client.CreateDatabase({ 'id': DOCUMENTDB_DATABASE })\nelse:\n    db = read_database(client, DOCUMENTDB_DATABASE)\n# コレクションのオプションを作成(RUsが11000となっているので注意！)\noptions = {\n    'offerThroughput': 11000\n}\n\n# コレクションの作成\ncollection_definition = { 'id': DOCUMENTDB_COLLECTION, 'partitionKey': {'paths': ['/id'],'kind': 'Hash'} }\nif find_collection(client,DOCUMENTDB_DATABASE,  DOCUMENTDB_COLLECTION) ==False:\n    collection = client.CreateCollection(db['_self'], collection_definition, options)\nelse:\n    collection = read_collection(client, DOCUMENTDB_DATABASE, DOCUMENTDB_COLLECTION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "secrets = {\n  \"Endpoint\": endpoint,\n  \"Masterkey\": master_key,\n  \"Database\": DOCUMENTDB_DATABASE,\n  \"Collection\": DOCUMENTDB_COLLECTION,\n  \"Upsert\": \"true\"\n}\nwith open(secrets_path, \"w\") as file:\n    json.dump(secrets, file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2 トレーニング\n\n次に、ここでは[MovieLens](https://grouplens.org/datasets/movielens/)のデータセットを使用して、[Alternating Least Squares model (交互最小二乗法モデル)](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html)におけるトレーニングを行います。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# トップ k 個のアイテムをレコメンド\nTOP_K = 100\n\n# Movielens のデータサイズを指定: 100k, 1m, 10m, 20m or 27m\nMOVIELENS_DATA_SIZE = '1m'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.1 MovieLens データセットのダウンロード"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# MOVIELENS_DATA_SIZE の指定によってダウンロードするファイルを変更。未指定時は 100k を選択\nif MOVIELENS_DATA_SIZE == '100k':\n  input_file_name = \"ml-100k.csv\"\nelif MOVIELENS_DATA_SIZE == '1m':\n  input_file_name = \"ml-1m.csv\"\nelif MOVIELENS_DATA_SIZE == '10m':\n  input_file_name = \"ml-10m.csv\"\nelif MOVIELENS_DATA_SIZE == '20m':\n  input_file_name = \"ml-20m.csv\"\nelif MOVIELENS_DATA_SIZE == '27m':\n  input_file_name = \"ml-27m.csv\"\nelse:\n  input_file_name = \"ml-100k.csv\"\n\n# コンテナ名とファイル形式を定義（カンマ区切り、タブ区切りはどちらも csv を指定）\ninput_container_location = \"wasbs://<container name>@<Azure Storage Account Name>.blob.core.windows.net/\"\ninput_file_location =  input_container_location + input_file_name\nfile_type = \"csv\"\n\n# 注: データフレームベース API for ALS は現在、整数のユーザー、アイテムIDのみをサポートします。\nschema = StructType(\n    (\n        StructField(\"UserId\", IntegerType()),\n        StructField(\"MovieId\", IntegerType()),\n        StructField(\"Rating\", FloatType()),\n        StructField(\"Timestamp\", LongType()),\n    )\n)\n\n# CSV の区切り文字を変更する場合には、.option(\"delimiter\", \"\\t\") のように指定する\ndata = spark.read.format(file_type).schema(schema).option(\"header\", \"true\").load(input_file_location)\ndata.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.2 トレーニング、テスト用のデータに分割\nデータを分割するにはランダム、年代順、層別など、いくつかの方法があり、実世界ではユースケースによって適合するものが異なります。この例ではランダムに分割しますが、分割についての詳細は[こちらのガイド](https://github.com/Microsoft/Recommenders/blob/master/notebooks/01_data/data_split.ipynb)を参照してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# （訳注）トレーニング、テスト用に 75:25 の比率で分割。ランダムシードが 123 で設定されているため、毎回同じ分割方法になります\ntrain, test = spark_random_split(data, ratio=0.75, seed=123)\nprint (\"N train\", train.cache().count())\nprint (\"N test\", test.cache().count())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.3 トレーニングデータで ALS モデルをトレーニングし、テストデータでトップ k 個のレコメンドを取得\n\n映画の評価を予測するには、ユーザーの明示的なフィードバックとセットとなった評価データを使用します。モデルの推論を行うためのハイパーパラメータについては、[このページ](http://mymedialite.net/examples/datasets.html) の内容に基づいています。\n\nほとんどの状況下では、ハイパーパラメーターの探索と、いくつかの条件に基づいて最適な設定を選択したいと考えるでしょう。このプロセスの詳細については、[ここ](../04_model_select_and_optimize/hypertune_spark_deep_dive.ipynb) にある Deep Dive の追加情報を参照してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "header = {\n    \"userCol\": \"UserId\",\n    \"itemCol\": \"MovieId\",\n    \"ratingCol\": \"Rating\",\n}\n\n\nals = ALS(\n    rank=10,\n    maxIter=15,\n    implicitPrefs=False,\n    alpha=0.1,\n    regParam=0.05,\n    coldStartStrategy='drop',\n    nonnegative=True,\n    **header\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ALSのトレーニング実行\nmodel = als.fit(train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "映画のレコメンドのユース ケースでは、ユーザー自身が評価を行った映画のレコメンドは意味がありません。したがって、評価済みの映画はレコメンドのアイテムから削除されます。\n\nこれを実行するためには、すべてのユーザーにすべての映画をお勧めし、トレーニング データセットに存在するユーザーと映画の組み合わせを削除します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 全てのユーザーとアイテムのペアとスコアのクロスジョインを取得\nusers = train.select('UserId').distinct()\nitems = train.select('MovieId').distinct()\nuser_item = users.crossJoin(items)\ndfs_pred = model.transform(user_item)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dfs_pred.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 表示済みアイテムの除去\ndfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n    train.alias(\"train\"),\n    (dfs_pred['UserId'] == train['UserId']) & (dfs_pred['MovieId'] == train['MovieId']),\n    how='outer'\n)\n\ntop_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[\"train.Rating\"].isNull()) \\\n    .select('pred.' + 'UserId', 'pred.' + 'MovieId', 'pred.' + \"prediction\")\n\ntop_all.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.4 ALSのパフォーマンス評価\n\nモデルのパフォーマンスを、Precision@K, Recall@K, [MAP](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\) または [nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) のような指標で評価します。レコメンデーションについてどの評価指標が最適かを検討するには、[このガイド](https://github.com/Microsoft/Recommenders/blob/master/notebooks/03_evaluate/evaluation.ipynb)を参照してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=\"UserId\", col_item=\"MovieId\", \n                                    col_rating=\"Rating\", col_prediction=\"prediction\", \n                                    relevancy_method=\"top_k\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ランキング指標の評価\n\nprint(\"Model:\\tALS\",\n      \"Top K:\\t%d\" % rank_eval.k,\n      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 評価指標の評価\n\nprediction = model.transform(test)\nrating_eval = SparkRatingEvaluation(test, prediction, col_user=\"UserId\", col_item=\"MovieId\", \n                                    col_rating=\"Rating\", col_prediction=\"prediction\")\n\nprint(\"Model:\\tALS rating prediction\",\n      \"RMSE:\\t%.2f\" % rating_eval.rmse(),\n      \"MAE:\\t%f\" % rating_eval.mae(),\n      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.5 モデルの保存"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.write().overwrite().save(model_name)\nmodel_local = \"file:\" + os.getcwd() + \"/\" + model_name\ndbutils.fs.cp(model_name, model_local, True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. レコメンデーション サービスを運用化する\n望ましいパフォーマンスを持つモデルの構築後は、リアルタイム サービスによって利用される REST エンドポイントとして機能します。ここでは [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/), [Azure Machine Learning Service](https://azure.microsoft.com/en-us/services/machine-learning-service/) と [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes) を使用してレコメンデーション サービスを運用します。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.1 Cosmos DB 内にレコメンデーションのルックアップを作成する\n\n最初に、モデルによって予測された各ユーザーのトップ 10 のレコメンドは、Cosmos DB でルックアップ テーブルとして格納されます。実行時に、サービスは Cosmos DB に格納されている事前計算済みのトップ 10 のレコメンドを返します:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(secrets_path) as json_data:\n    writeConfig = json.load(json_data)\n    recs = model.recommendForAllUsers(10)\n    recs.withColumn(\"id\",recs[userCol].cast(\"string\")).select(\"id\", \"recommendations.\"+ itemCol)\\\n    .write.format(\"com.microsoft.azure.cosmosdb.spark\").mode('overwrite').options(**writeConfig).save()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Azure Machine Learning の構成\n\n次に、Azure Machine Learning Service を使用してモデルのスコアリングイメージを作成し、Azure Kubernetes Service にスケーラブルなコンテナ サービスとして展開します。これを実現するには、**スコアリング スクリプト** と **環境設定** を作成する必要があります。2 つのファイルの内容を次に示します。\n\nスコアリング スクリプトでは、ユーザー IDの入力を与えられたトップ 10 のレコメンドされた映画をルックアップするために Cosmos DB を呼び出します:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "score_sparkml = \"\"\"\n\nimport json\ndef init(local=False):\n    global client, collection\n    try:\n      # Query them in SQL\n      import pydocumentdb.document_client as document_client\n\n      MASTER_KEY = '{key}'\n      HOST = '{endpoint}'\n      DATABASE_ID = \"{database}\"\n      COLLECTION_ID = \"{collection}\"\n      database_link = 'dbs/' + DATABASE_ID\n      collection_link = database_link + '/colls/' + COLLECTION_ID\n      \n      client = document_client.DocumentClient(HOST, {'masterKey': MASTER_KEY})\n      collection = client.ReadCollection(collection_link=collection_link)\n    except Exception as e:\n      collection = e\ndef run(input_json):      \n\n    try:\n      import json\n\n      id = json.loads(json.loads(input_json)[0])['id']\n      query = {'query': 'SELECT * FROM c WHERE c.id = \"' + str(id) +'\"' } #+ str(id)\n\n      options = {'partitionKey':str(id)}\n      document_link = 'dbs/{DOCUMENTDB_DATABASE}/colls/{DOCUMENTDB_COLLECTION}/docs/{0}'.format(id)\n      result = client.ReadDocument(document_link, options);\n  \n    except Exception as e:\n        result = str(e)\n    return json.dumps(str(result)) #json.dumps({{\"result\":result}})\n\"\"\"\n\n\nwith open(secrets_path) as json_data:\n    writeConfig = json.load(json_data)\n    score_sparkml = score_sparkml.replace(\"{key}\",writeConfig['Masterkey']).replace(\"{endpoint}\",writeConfig['Endpoint']).replace(\"{database}\",writeConfig['Database']).replace(\"{collection}\",writeConfig['Collection']).replace(\"{DOCUMENTDB_DATABASE}\",DOCUMENTDB_DATABASE).replace(\"{DOCUMENTDB_COLLECTION}\", DOCUMENTDB_COLLECTION)\n\n    exec(score_sparkml)\n\n    with open(\"score_sparkml.py\", \"w\") as file:\n        file.write(score_sparkml)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "次に、必要な依存関係が記載された環境設定ファイルを作成します:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile myenv_sparkml.yml\n\nname: myenv\nchannels:\n  - defaults\ndependencies:\n  - pip:\n    - numpy==1.14.2\n    - scikit-learn==0.19.1\n    - pandas\n    - azureml-core\n    - pydocumentdb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "作成したモデルを登録します:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "mymodel = Model.register(model_path = model_name, # これはローカルのファイルを指定します\n                       model_name = model_name, # これはモデルとして登録されている名前で、パスと名前は同じものを使用します。\n                       description = \"ADB trained model\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 モデルをサービスとして AKS に展開"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.3.1 モデルサービス用にコンテナを作成:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Web サービス用のイメージを作成\nmodels = [mymodel]\nruntime = \"spark-py\"\nconda_file = 'myenv_sparkml.yml'\ndriver_file = \"score_sparkml.py\"\n\n# イメージの作成\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                    runtime = runtime, \n                                    conda_file = conda_file)\n\nimage = ContainerImage.create(name = container_image_name,\n                                # これはモデル オブジェクトです\n                                models = [mymodel],\n                                image_config = myimage_config,\n                                workspace = ws)\n\n# 作成プロセスの完了を待機\nimage.wait_for_creation(show_output = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.3.2 コンテナ実行用の AKS クラスタの作成 (このプロセスは 10 から 25分程度かかります):"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AksCompute, ComputeTarget\n\n# デフォルトの構成を使用 (パラメータを指定してカスタマイズ可能)\nprov_config = AksCompute.provisioning_configuration()\n\n# クラスタの作成\naks_target = ComputeTarget.create(workspace = ws, \n                                  name = aks_name, \n                                  provisioning_configuration = prov_config)\n\naks_target.wait_for_completion(show_output = True)\n\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.3.3 コンテナ イメージを AKS に展開:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Web サービスの構成を設定 (ここではデフォルト設定と App Insight の使用を指定)\naks_config = AksWebservice.deploy_configuration(enable_app_insights=True)\n\n# シングル コマンドを使用して Webservice を作成しますが、直接イメージを使用する方法もあります。\ntry:\n    aks_service = Webservice.deploy_from_image(\n      workspace=ws, \n      name=service_name,\n      deployment_config = aks_config,\n      image = image,\n      deployment_target = aks_target\n      )\n    aks_service.wait_for_deployment(show_output=True)\nexcept Exception:\n    aks_service = Webservice.list(ws)[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.4 AKS モデル サービスの呼び出し\n展開後、ユーザー ID を指定してサービスを呼び出すことができます - サービスは、Cosmos DB でそのユーザーのトップ 10 のレコメンドをルックアップし、結果を送り返します。\n次のスクリプトでは、レコメンデーション サービス API を呼び出し、特定のユーザー ID の結果を表示する方法を示しています:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scoring_url = aks_service.scoring_uri\nservice_key = aks_service.get_keys()[0]\n\ninput_data = '[\"{\\\\\"id\\\\\":\\\\\"123\\\\\"}\"]'.encode()\n\nreq = urllib.request.Request(scoring_url,data=input_data)\nreq.add_header(\"Authorization\",\"Bearer {}\".format(service_key))\nreq.add_header(\"Content-Type\",\"application/json\")\n\ntic = time.time()\nwith urllib.request.urlopen(req) as result:\n    res = result.readlines()\n    print(res)\n    \ntoc = time.time()\nt2 = toc - tic\nprint(\"Scoring URL : \", scoring_url)\nprint(\"Service Key : Bearer \", service_key)\nprint(\"Input data : \", input_data.decode())\nprint(type(input_data))\nprint(\"Full run took %.2f seconds\" % (toc - tic))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "ALS_Movie_Example",
    "notebookId": 1065982370657128
  },
  "nbformat": 4,
  "nbformat_minor": 1
}